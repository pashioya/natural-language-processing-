{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Intro: F-strings,textfiles  and regular expressions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### F-strings and zip"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We recap some important data manipulations stuff before we embark on our journey into Natural Language Processing.\n",
    "Let's start with f-strings. In a print statement we can use variables. E.g."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Bond , James Bond\n"
     ]
    }
   ],
   "source": [
    "surname = \"Bond\"\n",
    "name= \"James\"\n",
    "# in a print statement we can use variables like\n",
    "print(\"My name is\", surname, \",\",name,surname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Bond, James Bond\n"
     ]
    }
   ],
   "source": [
    "# or like\n",
    "print(\"My name is {0}, {1} {0}\".format(surname,name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But we prefer to use f-strings:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Bond, James Bond\n"
     ]
    }
   ],
   "source": [
    "print(f\"My name is {surname}, {name} {surname}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because you can place any Python expression in the curly braces. E.g. we can use a zip function:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bond drives a Sunbeam Alpine Serie 5 in the movie 'Doctor No'.\n",
      "Bond drives a Bentley MK IV in the movie 'From Russia with Love '.\n",
      "Bond drives a Range Rover Sport in the movie 'No Time To Die'.\n",
      "Bond drives a Ford Mustang in the movie 'Diamands Are Forever'.\n",
      "Bond drives a Aston Martin DB5 in the movie 'Tomorrow Never Dies'.\n",
      "Bond drives a Aston Martin DBS V12 in the movie 'Quantum of Solace'.\n"
     ]
    }
   ],
   "source": [
    "cars =[\"Sunbeam Alpine Serie 5\",\"Bentley MK IV\",\"Range Rover Sport\",\"Ford Mustang\",\"Aston Martin DB5\",\"Aston Martin DBS V12\"];\n",
    "movies=[\"Doctor No\",\"From Russia with Love \",\"No Time To Die\",\"Diamands Are Forever\",\"Tomorrow Never Dies\",\"Quantum of Solace\"];\n",
    "cm=zip(cars,movies)\n",
    "for car,movie in cm:\n",
    "    print(f\"{surname} drives a {car} in the movie '{movie}'.\");\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bond drives a Sunbeam Alpine Serie 5 in the movie 'Doctor No'.\n",
      "Bond drives a Bentley MK IV in the movie 'From Russia with Love '.\n",
      "Bond drives a Range Rover Sport in the movie 'No Time To Die'.\n",
      "Bond drives a Ford Mustang in the movie 'Diamands Are Forever'.\n",
      "Bond drives a Aston Martin DB5 in the movie 'Tomorrow Never Dies'.\n",
      "Bond drives a Aston Martin DBS V12 in the movie 'Quantum of Solace'.\n"
     ]
    }
   ],
   "source": [
    "pl=[print(f\"{surname} drives a {car} in the movie '{movie}'.\") for car,movie in zip(cars,movies)];"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'zip'>\n"
     ]
    }
   ],
   "source": [
    "# what is the zip object?\n",
    "print(type(cm))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#how to convert to a tuple?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "cmt=tuple(cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cmt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#what actors where playing in the different movies?\n",
    "actors=[\"Sean Connery\",\"Sean Connery \",\"Daniel Craig\",\"Sean Connery\",\"Pierce Brosman\",\"Daniel Craig\"];"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "acm=list(zip(actors, cars, movies));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actor', 'car', 'movie'), ('Sean Connery', 'Sunbeam Alpine Serie 5', 'Doctor No'), ('Sean Connery ', 'Bentley MK IV', 'From Russia with Love '), ('Daniel Craig', 'Range Rover Sport', 'No Time To Die'), ('Sean Connery', 'Ford Mustang', 'Diamands Are Forever'), ('Pierce Brosman', 'Aston Martin DB5', 'Tomorrow Never Dies'), ('Daniel Craig', 'Aston Martin DBS V12', 'Quantum of Solace')]\n"
     ]
    }
   ],
   "source": [
    "#acm2=list(acm);\n",
    "#print(acm2)\n",
    "acm.insert(0,(\"actor\",\"car\",\"movie\"));\n",
    "print(acm)\n",
    "#a.insert(0,('actor','car','movie'))\n",
    "#print(a)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor car movie\n",
      "Sean Connery Sunbeam Alpine Serie 5 Doctor No\n",
      "Sean Connery  Bentley MK IV From Russia with Love \n",
      "Daniel Craig Range Rover Sport No Time To Die\n",
      "Sean Connery Ford Mustang Diamands Are Forever\n",
      "Pierce Brosman Aston Martin DB5 Tomorrow Never Dies\n",
      "Daniel Craig Aston Martin DBS V12 Quantum of Solace\n"
     ]
    }
   ],
   "source": [
    "# output like this is messy\n",
    "pl=[print(f\"{actor} {car} {movie}\") for actor,car,movie in acm];"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor           car                       movie               \n",
      "Sean Connery    Sunbeam Alpine Serie 5    Doctor No           \n",
      "Sean Connery    Bentley MK IV             From Russia with Love \n",
      "Daniel Craig    Range Rover Sport         No Time To Die      \n",
      "Sean Connery    Ford Mustang              Diamands Are Forever\n",
      "Pierce Brosman  Aston Martin DB5          Tomorrow Never Dies \n",
      "Daniel Craig    Aston Martin DBS V12      Quantum of Solace   \n"
     ]
    }
   ],
   "source": [
    "# we can use padding in combination with the variables.E.g.\n",
    "pl=[print(f\"{actor:15} {car:25} {movie:20}\") for actor,car,movie in acm];"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text files: reading and writing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### txt files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by storing our bond movie information.\n",
    "next we use the magic command %%writefile (works only in Jupyter notebook)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting myfile.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile myfile.txt\n",
    "actor           car                       movie\n",
    "Sean Connery    Sunbeam Alpine Serie 5    Doctor No\n",
    "Sean Connery    Bentley MK IV             From Russia with Love\n",
    "Daniel Craig    Range Rover Sport         No Time To Die\n",
    "Sean Connery    Ford Mustang              Diamands Are Forever\n",
    "Pierce Brosman  Aston Martin DB5          Tomorrow Never Dies\n",
    "Daniel Craig    Aston Martin DBS V12      Quantum of Solace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/ronybaekeland/PycharmProjects/pythonProject/TextBasedInformationRetrieval/Code'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #if we want to know the working directory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# edit the path ub the following line to your situation\n",
    "bondfile=open('/Users/ronybaekeland/PycharmProjects/pythonProject/TextBasedInformationRetrieval/Code/myfile.txt');\n",
    "mf=bondfile.read();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor           car                       movie\n",
      "Sean Connery    Sunbeam Alpine Serie 5    Doctor No\n",
      "Sean Connery    Bentley MK IV             From Russia with Love\n",
      "Daniel Craig    Range Rover Sport         No Time To Die\n",
      "Sean Connery    Ford Mustang              Diamands Are Forever\n",
      "Pierce Brosman  Aston Martin DB5          Tomorrow Never Dies\n",
      "Daniel Craig    Aston Martin DBS V12      Quantum of Solace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "mf2=bondfile.read();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# what will be printed. Why?\n",
    "print(mf2)\n",
    "# how to fix this?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "bondfile.seek(0);# we move the cursor back to the beginning\n",
    "mf2=bondfile.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor           car                       movie\n",
      "Sean Connery    Sunbeam Alpine Serie 5    Doctor No\n",
      "Sean Connery    Bentley MK IV             From Russia with Love\n",
      "Daniel Craig    Range Rover Sport         No Time To Die\n",
      "Sean Connery    Ford Mustang              Diamands Are Forever\n",
      "Pierce Brosman  Aston Martin DB5          Tomorrow Never Dies\n",
      "Daniel Craig    Aston Martin DBS V12      Quantum of Solace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mf2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile text2.txt\n",
    "this is line nr 1\n",
    "Next we have line 2\n",
    "etc..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# to read and (over)write we use w+ option\n",
    "otherfile1=open('anotherfile.txt','w+')\n",
    "otherfile1.write('we write something on this file')\n",
    "whatsinf= otherfile1.read()\n",
    "print(whatsinf)\n",
    "#why we do not see ' we write something on this file'?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we write something on this file\n"
     ]
    }
   ],
   "source": [
    "otherfile1.seek(0)\n",
    "whatsinf= otherfile1.read()\n",
    "print(whatsinf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we write something else on this file\n"
     ]
    }
   ],
   "source": [
    "# to read and (over)write we use w+ option\n",
    "otherfile1=open('anotherfile.txt','w+')\n",
    "otherfile1.write('we write something else on this file')\n",
    "otherfile1.seek(0)\n",
    "whatsinf= otherfile1.read()\n",
    "print(whatsinf)\n",
    "#what do you see?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is line nr 1\n",
      "Next we have line 2\n",
      "etc...\n",
      "we write something at the end on this file\n"
     ]
    }
   ],
   "source": [
    "# if we want to append something to text2.txt we use something different, 'a+' append +read mode\n",
    "otherfile2=open('text2.txt','a+')\n",
    "otherfile2.write('we write something at the end on this file')\n",
    "otherfile2.seek(0)\n",
    "whatsinf= otherfile2.read()\n",
    "print(whatsinf)\n",
    "#what do you see?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose we stop now. Is this ok?\n",
    "What do we need to do?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "otherfile1.close()\n",
    "otherfile2.close()\n",
    "bondfile.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A good habit is to use a contextmanager which prevent you to forget to close the files. For example:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is line nr 1\\n', 'Next we have line 2\\n', 'etc...\\n', 'we write something at the end on this file']\n"
     ]
    }
   ],
   "source": [
    "with open('text2.txt') as example:\n",
    "    mytext= example.readlines();\n",
    "print(mytext)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A lot of documents that we encounter are in PDF format.\n",
    "Next we will learn to open en read PDF files (here we only consider PDF created by a wordprocessor (Acrobat, Word,..)).\n",
    "The package PyPDF2 will enable us to manage PDF files. Apart from opening, closing, reading and writing it also provide some useful methods and functions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PDF files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See discussions, stats, and author profiles for this publication at: \n",
      "https://www.researchgate.net/publication/333752364\n",
      "What Is Data Science?\n",
      "Chapter\n",
      " · June 2019\n",
      "DOI: 10.1007/978-3-030-11821-1_8\n",
      "CITATIONS\n",
      "72\n",
      "\n",
      "READS\n",
      "13,029\n",
      "1 author:\n",
      "Some of the authors of this publication are also working on these related projects:\n",
      "\n",
      "Application - Database Co-Evolution\n",
      " \n",
      "View project\n",
      "\n",
      "The Data Science Reference Framework\n",
      " \n",
      "View project\n",
      "\n",
      "\n",
      "Michael L. Brodie\n",
      "Harvard University\n",
      "174\n",
      " \n",
      "PUBLICATIONS\n",
      "   \n",
      "3,601\n",
      " \n",
      "CITATIONS\n",
      "   \n",
      "SEE PROFILE\n",
      "All content following this page was uploaded by \n",
      "Michael L. Brodie\n",
      " on 18 September 2019.\n",
      "The user has requested enhancement of the downloaded file.\n"
     ]
    }
   ],
   "source": [
    "import  PyPDF2\n",
    "MyDSfile=open('WhatisDataScienceFinalMay162018.pdf',mode='rb') # we read in as a binary file because we deal here with a PDF FILE\n",
    "PDFreader=PyPDF2.PdfFileReader(MyDSfile)\n",
    "# several methods are at our disposal to access the PDFreader document. E.g.\n",
    "page0=PDFreader.getPage(0) # store first page  in the variable page0\n",
    "text0=page0.extractText() # to access the content of page 1\n",
    "print(text0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "22"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDFreader.numPages #to see the number of pages in the PDF document\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "MyDSfile.close() #never forget to close the file\n",
    "#How can we add pages to a PDF document?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How can we change the PDF document?\n",
    "Suppose we only are interested in the real content of the article \"What is DataScience ?\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is Data Science?  \n",
      "Michael L. Brodie,  Computer Science and  Artificial Intelligence Laboratory, MIT  \n",
      " \n",
      "Abstract   \n",
      "Data  Science,  a  new  discovery paradigm, is  potentially  one  of  the most  significant  advances  of \n",
      "the early 21st\n",
      " century. Originating in scientific discovery, it is being applied to every human endeavor for \n",
      "which there is adequate data. While remarkable successes have been achieved, even greater claims have \n",
      "been made. Benefits, challenge, and  risks abound. The scien ce underlying  data science has yet to emerge. \n",
      "Maturity  is  more  than  a  decade  away.  This  claim  is  based  firstly  on  observing  the  centuries-long \n",
      "developments of its predecessor paradigms  – empirical, theoretical, and Jim Gray’s  Fourth  Paradigm  of \n",
      "Scientific  Discovery  (Hey, Tansley & Tolle, 2009)  (aka eScience, data -intensive, computational, procedural); \n",
      "and secondly on my studies of over 1 50 data science use cases, several data science-based startups, and, \n",
      "on  my  scientific  advisory  role for Insight 1\n",
      ", a Data  Science  Research  Institute  (DSRI)  that  requires  that  I \n",
      "understand the opportunities, state of the art, and research challenges for the emerging discipline of data \n",
      "science. This chapter addresses e ssential questions for a DSRI: What is data science? and What is world-\n",
      "class data science research ? A companion chapter  On Developing Data Sci ence (Brodie, 2018b) addresses \n",
      "the development of data science applications and of the data science discipline itself.  \n",
      "1 Introduction  \n",
      "What  can  data  science  do? What  characteristics  distinguish  data  science  from  previous  scientific \n",
      "discovery  paradigms ? What  are  the  methods  for  conducting  data  science? What  is  the  impact  of  data \n",
      "science? This chapter offers initial answers to th ese and related questions . A companion chapter  (Brodie, \n",
      "2018b) addresses  the  development of  data  science  as  a  discipline,  as  a  methodology,  as  well  as  data \n",
      "science research and education.  Let’s start with some slightly provocative claims concerning data science.  \n",
      "Data science has been used successfully  to accelerate discovery of probabilistic outcomes in many \n",
      "domains. Piketty’s (2014) monumental result on wealth and income inequality was achieved through data \n",
      "science. It used over 120 years of sporadic, incomplete, observational economic data, collected  over ten \n",
      "years from all over the world (Brodie, 2014b). What is now called  computational economics  was used to \n",
      "establish  the  correlation, with  a  very  high  likelihood (0.90),  that  wealth gained  from  labor  could  never \n",
      "keep  up  with  wealth  gained  from  assets. What  made  front  page  news  worldwide  was  a  second,  more \n",
      "dramatic correlation that there is a perpetual and growing wealth gap between the rich and the poor. This \n",
      "second  correlation  was  not  derived  by  data  analysis  but  is  a  human  interpretation  of  Piketty’s data \n",
      "analytic result. It contributed to making  Capital  in  the  21st  Century the best-selling book on economics, \n",
      "but possibly the least read. Within a year, the core result was verified by independent analyses to a far \n",
      "greater  likelihood  (0.99).  One  might  expect  that  further  confirmation  of  Piketty’s  finding  would  be \n",
      "newsworthy;  however,  it  was  not  as  the  more  dramatic  rich-poor  correlation,  while  never  analytically \n",
      "established had far greater appeal. This illustrates the benefits and risks of data science.  \n",
      "Frequently,  due  to  the  lack  of  evidence,  economic  theories  fail.  Matthew  Weinzierl,  a  leading \n",
      "Harvard  University economist,  questions  such  economic modelling  in  general  saying, “that  the  world is \n",
      "too complicated to be modelled with anything like perfect acc uracy” and \"Used in isolation, however, it \n",
      "can lead to trouble” ( Economist, February 2018). Reputedly, Einstein said: “Not everything that counts can \n",
      "be  counted.  Not  everything  that’s  counted,  counts”.  The  hope  is  that  data  science  and  computational \n",
      "econom ics  will  provide  theories  that  are  fact-based  rather  than  based  on  hypotheses  of  “expert” \n",
      "economists  (Economist,  January  2018)  leading  to  demonstrably  provable  economic  theories,  i.e.,  what \n",
      "really happened or will happen. This chapter suggests that this ho pe will not be realized this year.   \n",
      "                                                               \n",
      "1\n",
      " https://www.insight -centre.org/ \n"
     ]
    }
   ],
   "source": [
    "MyDSfile=open('WhatisDataScienceFinalMay162018.pdf',mode='rb') # we read in as a binary file because we deal here with a PDF FILE\n",
    "PDFreader=PyPDF2.PdfFileReader(MyDSfile)\n",
    "# several methods are at our disposal to access the PDFreader document. E.g.\n",
    "# getPage(1) will show the second page and if you run this we see that her the real article starts\n",
    "page1=PDFreader.getPage(1) # store page 1 (second page) in the variable page1\n",
    "text1=page1.extractText()\n",
    "print(text1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercise: Make a a PDF document with all the pages from \"What is Data Science?\" except the first one. Further make a list which also contains the different pages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[' What is Data Science?  \\nMichael L. Brodie,  Computer Science and  Artificial Intelligence Laboratory, MIT  \\n \\nAbstract   \\nData  Science,  a  new  discovery paradigm, is  potentially  one  of  the most  significant  advances  of \\nthe early 21st\\n century. Originating in scientific discovery, it is being applied to every human endeavor for \\nwhich there is adequate data. While remarkable successes have been achieved, even greater claims have \\nbeen made. Benefits, challenge, and  risks abound. The scien ce underlying  data science has yet to emerge. \\nMaturity  is  more  than  a  decade  away.  This  claim  is  based  firstly  on  observing  the  centuries-long \\ndevelopments of its predecessor paradigms  – empirical, theoretical, and Jim Gray’s  Fourth  Paradigm  of \\nScientific  Discovery  (Hey, Tansley & Tolle, 2009)  (aka eScience, data -intensive, computational, procedural); \\nand secondly on my studies of over 1 50 data science use cases, several data science-based startups, and, \\non  my  scientific  advisory  role for Insight 1\\n, a Data  Science  Research  Institute  (DSRI)  that  requires  that  I \\nunderstand the opportunities, state of the art, and research challenges for the emerging discipline of data \\nscience. This chapter addresses e ssential questions for a DSRI: What is data science? and What is world-\\nclass data science research ? A companion chapter  On Developing Data Sci ence (Brodie, 2018b) addresses \\nthe development of data science applications and of the data science discipline itself.  \\n1 Introduction  \\nWhat  can  data  science  do? What  characteristics  distinguish  data  science  from  previous  scientific \\ndiscovery  paradigms ? What  are  the  methods  for  conducting  data  science? What  is  the  impact  of  data \\nscience? This chapter offers initial answers to th ese and related questions . A companion chapter  (Brodie, \\n2018b) addresses  the  development of  data  science  as  a  discipline,  as  a  methodology,  as  well  as  data \\nscience research and education.  Let’s start with some slightly provocative claims concerning data science.  \\nData science has been used successfully  to accelerate discovery of probabilistic outcomes in many \\ndomains. Piketty’s (2014) monumental result on wealth and income inequality was achieved through data \\nscience. It used over 120 years of sporadic, incomplete, observational economic data, collected  over ten \\nyears from all over the world (Brodie, 2014b). What is now called  computational economics  was used to \\nestablish  the  correlation, with  a  very  high  likelihood (0.90),  that  wealth gained  from  labor  could  never \\nkeep  up  with  wealth  gained  from  assets. What  made  front  page  news  worldwide  was  a  second,  more \\ndramatic correlation that there is a perpetual and growing wealth gap between the rich and the poor. This \\nsecond  correlation  was  not  derived  by  data  analysis  but  is  a  human  interpretation  of  Piketty’s data \\nanalytic result. It contributed to making  Capital  in  the  21st  Century the best-selling book on economics, \\nbut possibly the least read. Within a year, the core result was verified by independent analyses to a far \\ngreater  likelihood  (0.99).  One  might  expect  that  further  confirmation  of  Piketty’s  finding  would  be \\nnewsworthy;  however,  it  was  not  as  the  more  dramatic  rich-poor  correlation,  while  never  analytically \\nestablished had far greater appeal. This illustrates the benefits and risks of data science.  \\nFrequently,  due  to  the  lack  of  evidence,  economic  theories  fail.  Matthew  Weinzierl,  a  leading \\nHarvard  University economist,  questions  such  economic modelling  in  general  saying, “that  the  world is \\ntoo complicated to be modelled with anything like perfect acc uracy” and \"Used in isolation, however, it \\ncan lead to trouble” ( Economist, February 2018). Reputedly, Einstein said: “Not everything that counts can \\nbe  counted.  Not  everything  that’s  counted,  counts”.  The  hope  is  that  data  science  and  computational \\neconom ics  will  provide  theories  that  are  fact-based  rather  than  based  on  hypotheses  of  “expert” \\neconomists  (Economist,  January  2018)  leading  to  demonstrably  provable  economic  theories,  i.e.,  what \\nreally happened or will happen. This chapter suggests that this ho pe will not be realized this year.   \\n                                                               \\n1\\n https://www.insight -centre.org/ ',\n 'Many  such  outcomes2\\n have  led  to  verified  results  through  methods  outside  data  science.  Most \\ncurrent data analyses are domain specific, many even specific to classes of models, classes of analytical \\nmethods, and specific pipelines. Few data science methods have been generaliz ed outside their original \\ndomains  of  application,  let  alone  to  all  domains  (to  illustrated  in  a  moment).  A  rare  and  excellent \\nexception  is  a  generic  scientific  discovery  method  over  scientific  corpora  (Nagarajan  et.  al.,  2015) \\ngeneralized from a specific m ethod over medical corpora developed for drug discovery (Spangler et. al., \\n2014) that is detailed later in the chapter.   \\nIt is often claimed that data science will transform conventional disciplines. While transformations \\nare  underway  in  many  areas,  including  supply  chain  management3\\n (Waller  and  Fawcett,  2013)  and \\nchemical engineering (Data Science, 2018), only time and concrete results will tell the extent and value of \\nthe transformations. The companion chapter  On Developing Data Science (Brodie, 2018b) di scusses with \\nthe transformation myth.   \\nWhile  there  is  much  science  in many  domain -specific  data  science  activities,  there  is  little \\nfundamental  science  that  is  applicable  across  domains.  To  warrant  the  designation  data  science,  this \\nemerging  paradigm  requi res  fundamental  principles  and  techniques  applicable  to  all  relevant  domains, \\njust  as  the scientific  principles  of  the  scientific  method  apply  across  many  domains.  Since  most  data \\nscience work is domain specific, often model - and method -specific, data science does not yet warrant the \\ndesignation as a science.  \\nThis  chapter  explores  the  current  nature  of  data  science,  its  qualitative  differences  with  its \\npredecessor  scientific  discovery  paradigms,  its  core  value  and  components  that,  when  mature,  would \\nwarrant the designation  data science . Descriptions of large-scale data science activities referenced in this \\nchapter apply, scaled down, to data science activities of all sizes, including increasingly ubiquitous desktop \\ndata analytics in business.  \\n2 What is data science?  \\nDue to its remarkable popularity, there is a ple thora of descriptions of data science, for example:  \\n \\nData Science is concerned with analyzing data and extracting useful knowledge from it. \\nBuilding predictive models is usually the most important activity for a Data Scientist 4\\n. \\n  \\nData Science is concerned  with analyzing Big Data to extract correlations with estimates \\nof likelihood and error . (Brodie, 2015a) \\n  \\nData  science  is  an  emerging  discipline  that  draws  upon  knowledge  in  statistical \\nmethodology  and  computer  science  to  create  impactful  predictions  and insights  for  a \\nwide range of traditional scholarly fields 5\\n. \\n  \\nDue  to  data  science  being  in  its  infancy,  these  descriptions  reflect  some  of  the  many  contexts  in \\nwhich it is used. This is both natural and appropriate for an emerging discipline that involves  many distinct \\ndisciplines and applications. A definition of data science requires the necessary and sufficient conditions \\nthat distinguish it from all other activities. While such a definition is premature, a working definition can \\nbe useful for discussion . The following definition is intended to explore the nature of this remarkable new \\n                                                               \\n2\\n Not Piketty’s, since computational economics can find  what might have happened - patterns, each with \\na given likelihood  - but lacks the means of establishing causal relationships, i.e., establishing  why, based \\nsolely on observational data.  \\n3\\n Selecting the best delivery route for 25 packages from 15 septillion alternatives, an ideal data science \\napplication, may explain the some of  the $1.3trn to $2trn a year in economic value projected to be gained \\nin  the  transformation  of  the  supply  chain  industry  due  to  AI-based  data  analytics  (Economist,  March \\n2018). \\n4\\n Gregory Piatetsky, KDnuggets,  https://www.kdnuggets.com/tag/data -science \\n5\\n Harvard Data Science Initiative  https://datascience.harvard.edu  ',\n 'discovery paradigm. It is based on studying over 150 data science use cases and benefits from three years \\nresearch and experience over a previous version (Brodie, 2015a). L ike many data science definitions, it will \\nbe improved over the next decade in which data science will mature and gain the designation as a new \\nscience. \\n \\nData  Science  is a body  of  principles  and  techniques  for  applying  data  analytic \\nmethods  to  data  at  scale,  including  volume,  velocity,  and  variety,  to  accelerate  the \\ninvestigation of phenomena represented by the data, by acquiring data, preparing and \\nintegrating  it,  possibly  integrated  with  existing  data,  to  discover  correlations  in  the \\ndata, with measures o f likelihood and within error bounds. Results are interpreted with \\nrespect to some predefined (theoretical, deductive, top -down) or emergent (fact-based, \\ninductive,  bottom-up)  specification  of  the  properties  of  the  phenomena  being \\ninvestigated.  \\n \\nA simple example of a data science analysis is the pothole detector developed at MIT (Eriksson \\net. al., 2008) to identify potholes on the streets of Cambridge, MA. The data was from inexpensive GPS \\nand  accelerometer  devices  placed  in  a fleet  of  taxis that drive  over Cambridge  streets.  The model  was \\ndesigned  ad  hoc for  this  application.  A  model  consists  of  the  features  (i.e.,  variables)  essential  to  the \\nanalysis  and  the relationships  amongst  the  features.  It  was  developed in  this  case ad  hoc by  the  team \\niteratively re fining the model through imagination, observation, and analysis. Ultimately, it consisted of a \\nlarge number of movement signatures, i.e., model features, each designed to detect specific movement \\ntypes that may indicate potholes and non -potholes, e.g., man holes, railroad tracks 6\\n, doors opening and \\nclosing, stopping, starting, accelerating, etc. Additionally, the size of the pothole was estimated by the size \\nof  the  movement.  The  analytical  method  was  the  algorithmic  detection  and  filtering  of  non-pothole \\nsignatures leaving as a result those movements that correlate with potholes with an estimated severity, \\nlikelihood,  and  error  bound.  The  severity  and  likelihood  estimates  were  developed  ad  hoc based  on \\nverifying  some  portion  of  the  detected  movements  with  the corresponding  road  surfaces  thus \\ncontributing  to  estimating  the  likelihood  that  the  non-potholes  were  excluded,  and  potholes  were \\nincluded.  Error  bounds  were  based  on  the  precision  of  the  equipment,  e.g.,  motion  device  readings, \\nnetwork  communications,  data  errors,  etc.  The  initial  result  was  many  thousands  of  locations  with \\nestimated severities, likelihoods, and error bounds. Conversion of likely pothole locations (correlations) to \\nactual potholes severe enough to warrant repair (causal relationships betw een movements and potholes) \\nwere  estimated  by  a  manual  inspection  of  some  percentage  of  candidate  potholes.  The  data  from  the \\ninspection  of  the  actual  locations,  called  ground  truth,  was  used  to  verify  the  likelihood estimates  and \\nestablish a threshold abo ve which confidence in the existence of a pothole warranted sending out a repair \\ncrew  to  repair  the  pothole.  The  customer,  the  City  of  Cambridge,  MA,  was  given  a  list  of  these  likely \\npotholes.  \\nThe immediate value of the pothole detector was that it reduced  the search for potholes from \\nmanually inspecting 125 miles of roads and relying on citizen reports that takes months, to discovering \\n                                                               \\n6\\n The pothole models consist of a number of signature movements, i.e., abstractions used to represent \\nmovements of the taxi, only some of which are related to the road su rface. Each signature movement was \\ncreated using the data (variables or features) available from a smartphone including the clock for time, \\nthe GPS for geographic location (latitude and longitude), and the accelerometer to measure changes in \\nvelocity  along  the  x,  y,  and z  axes.  For  example,  the  taxi crossing  a  railroad  track  would result in  many \\nsignature  “single  tire  crossing  single  rail  line”  movements,  one  for  each  of  four  tires  crossing  each  of \\nseveral  rail  lines.  A  “single  tire  crossing  single  rail  line”  involves  a  sudden,  short  vertical  (x-axis) \\nacceleration  combined  with  a  short  lateral  (y-axis)  movement,  forward  or  backward,  with  little  or  no \\nlateral  (z-axis)  movement.  Discounting  the  railroad  crossing  as  a  pothole  involves  recognizing  a  large \\nnumber  of movements as a taxi is crossing a rail line  - all combinations of “single tire crossing single rail \\nline” forward or backward, at any speed, and at any angle  - to determine the corresponding staccato of \\nthe multiple single tire events over multiple lin es. The pothole model is clearly  ad hoc, in contrast to well \\nestablished models in physics and retail marketing.  ',\n 'likely, sever potholes within days of their creation. Since 2008, pothole detectors have been installed on \\ncity  vehicles  in  many  US  cities.  The  pothole  detector  team  created  Cambridge  Mobile  Telematics  that \\ndevelops applications for vehicles sensor data, e.g., they annually produce reports on distracted driving \\nacross  the  USA  based  on data  from  over  100  million  trips  (Cambridge  Mobile  Telematics,  2018).  While \\nthese applications were used initially by insurance companies they are part of the burgeoning domain of \\nautonomous vehicles and are being used by the US National Academy of Sciences (Dingus T.A., 2016) for \\ndriving safety.  \\n3 Data science is a new paradigm of discovery  \\nData  science  emerged  from,  and  has many commonalities  with, its  predecessor  paradigm,  the \\nscientific  method7\\n;  however,  they  differ  enough  for  data  science  to  be  considered  a  distinct,  new \\nparadigm.  Like  the  scientific  method,  data  science  is  based  on  principles  and  techniques  required  to \\nconduct discovery activities that are typically defined in terms of a seque nce of steps, called a workflow or \\npipeline; results are specified probabilistically and with error bounds based on the data, the model, and \\nthe analytical method used; and the results are interpreted in terms of the hypothesis being evaluated, \\nthe  model, the  methods,  and  the  probabilistic  outcome  relative  to  the  accepted  requirements  of  the \\ndomain of the study. In both paradigms,  models  are collections of features (represented by variables that \\ndetermine the data to be collected) that characterize the esse ntial properties of the phenomenon being \\nanalyzed. Data corresponding to the features (variables) in the model are collected from real instances of \\nthe phenomena and analyzed using analytical methods developed for the type of analysis to be conducted \\nand  the  nature  of  the  data  collected,  e.g.,  different  methods  are  required  for  integers  uniformly \\ndistributed in time versus real numbers skewed due to properties of the phenomenon. The outcomes of \\nthe analysis are interpreted in terms of the phenomena being an alyzed within bounds of precision and \\nerrors that result from the data, model, and method compared with the precision required in the domain \\nbeing analyzed, e.g., particle physics requires precision of six standard deviations (six sigma). Data science \\ndiffers paradigmatically from the scientific method in data, models, methods, and outcomes, as described \\nbelow. Some differences may be due to data science being in its infancy, i.e., models for real -time cyber-\\nattacks may not yet have been developed and prove n; however, some differences, discussed below, are \\ninherent. We are in the process of learning which is which.  \\n3.1 Data science data, models, and methods   \\nData science data  is often obtained with limited knowledge of the conditions under which the \\ndata was generated, collected, and prepared for analysis, e.g., data found on the web; hence, it cannot be \\nevaluated  as  in  a  scientific  experiment  that  requires  precise  controls  on the  data.  Such  data  is  called \\nobservational. Compared with empirical scientific data, data science data is typically, but not necessarily, \\nat  scale  by  orders  of  magnitude  in  one  or  more  of  volume,  velocity,  and  variety.  Scale  requires \\nmanagement and analyt ic methods seldom required in empirical science.  \\nData science models  used in most scientific domains have long histories of development, testing, \\nand  acceptance,  e.g.,  the  standard  model  of  particle  physics8\\n emerged  in  1961  after  decades  of \\ndevelopment and  has matured over the subsequent decades. In contrast, currently data science models, \\ne.g.,  for  real-time  bidding  for  online  advertising,  are  created  on  demand  for  each  data  science  activity \\nusing many different, innovative, and  ad hoc methods. Once a mode l is proven, they can be accepted and \\n                                                               \\n7\\n The scientific method  is a body of  techniques  for investigating  phenomena , acquiring new  knowledge , or \\ncorrecting and integrating previous knowledge. To be termed scientific, a method of  inquiry  is commonly \\nbased on  empirical or measurable eviden ce subject to specific principles of reasoning.  \\nhttps://en.wikipedia.org/wiki/Scientific_method   \\n8\\n The Standard Model  of particle  physics is the theory describing three of the  four known  fundamental \\nforces (the electromagnetic , weak, and strong interactions,  and  not  including  the gravitational  force)  in \\nthe universe , as well as classifying all known elementary particles . It was developed in stages throughout \\nthe  latter  half  of  the  20th  century,  through  the  work  of  many  scientists  around  the  world. \\nhttps://en.wikipedia.org/wiki/Standard_Model   ',\n 'put into productive use with periodic tuning, e.g., real -time ad placement products. It is likely that many \\nproven  data  science  models  will  emerge  as  data  science  modelling  matures.  StackAdapt.com  has \\ndeveloped  such  a model  for  Real-time  Bidding  and  programmatic  ad  purchasing  (RTB)  that  is  its  core \\ncapability and intellectual property with which it has become a RTB world leader amongst 20 competitors \\nworldwide. The StackAdapt model is used to scan 10 BN data points a da y and manage up to 150,000 ad \\nopportunity requests per second during peak times.   \\n   Data science analytical methods , like data science models, are often domain - and  data-specific \\nand are developed exclusively for a specific data science activity. There are  generic methods, often named \\nby a class name. For example, the primary classes of Machine Learning algorithms 9\\n are: Linear Classifiers: \\nLogistic  Regression,  Naive  Bayes  Classifier;  Support  Vector  Machines;  Decision  Trees;  Boosted  Trees; \\nRandom Forest; Neu ral Networks; and Nearest Neighbor. There are generic algorithms for each class each \\nof  which  can  be  applied  in many domains.  However,  to  be  applied  in  a  specific  use  case  they must  be \\nrefined or tuned often to the point of being applicable in that use cas e only. This is addressed in the next \\nsection  that  questions  whether  there  are,  as  yet,  underlying,  thus  generalizable,  principles  in  data \\nscience.. \\nBoth models and methods require tuning or adjusting in time as more knowledge and data are \\nobtained. Empiri cal scientific models tend to evolve slowly, e.g., the standard model of particle physics is \\nmodified  slowly 10\\n; in  contrast,  data  science  models  typically  evolve rapidly  throughout  their  design  and \\ndevelopment,  and  even  in  deployment,  using  dynamic  learning.  Typically,  models  and  methods  are \\ntrained  using  semi-automatic  methods  by  which  specific  data  or  outcomes,  called  ground  truth,  are \\nconfirmed  by  humans  as  real  to  the  model  or  method.  More  automatic  methods,  e.g.,  reinforcement \\nlearning  and  meta-learning 11\\n,  are  being  developed  by  which  models  and  methods  are  created \\nautomatically (Silver, 2017).  \\n3.2 Data science fundamentals: Is data science a science?  \\n   Currently, most data science results are domain -, method -, and even data -specific. This raises the \\nquestion as to whether data science is yet a science, i.e., with generalizable results, or merely a collection \\nof  sophisticated  analytical  methods,  with,  as  yet,  a  few  underlying  principles  emerging,  such  as  Bayes\\' \\nTheorem, Uncle Bernie\\'s rule 12\\n,  and  Information  Bottleneck  theory.  The  scientific  method  is  defined  by \\nprinciples  that  ensure  scientific  objectivity,  such  as  empirical  design  and  the related  controls  to govern \\nexperimental  design  and  execution.  These  and  other  scientific  principles  make experiments  \"s cientific\", \\nthe  minimum  requirement  for  a  result  to  be  considered  scientific.  Scientific  experiments  vary  across \\ndomains, such as the statistical significance required in a given domain, e.g., two sigma has traditionally \\nbeen  adequate  in  many  domains  besides  particle  physics.  A  necessary,  defining  characteristic  of  data \\nscience  is  that  the  data  is  either  at  scale  (Big  Data)  or  observational  (collected  without  knowing  the \\nprovenance  - what controls were applied or with no controls uniformly applied) as is ge nerally the case in \\neconomics  and  social  sciences.  Under  those  conditions,  data  science  cannot  be  \"scientific\",  hence \\naccommodations must be made to draw conclusions from analysis over such data. As data science is just \\nemerging in each domain, we have few  principles or guidelines per domain, e.g., statistical significance of \\nresults,  or  across  all  domains,  e.g.,  the  extent  to  which  statistical  significance  is  required  in  any  data \\nscience  analysis.  The  above  mentioned  pothole  analysis  was  designed  and  executed  by  sheer  intuition \\nbeyond  the  general  ideas  of  identifying  the  hypothesis  (find  potholes  using  motion  devices  in  taxis), \\nexperimental design (put devices in taxis and record their signals), modeling (what features are critical), \\nand analysis (what moti ons indicate potholes, and which do not), and iteration of the model and analysis \\nuntil  acceptable  precision  was  reached.  The  pothole  data  science  activity  did  not  draw  on  previous \\nmethods, nor did it offer, i.e., was not cited, principles for modeling, me thods, or process.  \\n                                                               \\n9\\n https://medium.com/@sifium/machine -learning-types-of-classification-9497bd4f2e14   \\n10\\n Validating the Higgs -Boson took 49 years.  \\n11\\n http://bair.berkeley.edu/blog/2017/07/18/learning -to-learn/   \\n12\\n See Morgan, N., & Bourlard, H. (1990). Generalization and parameter estimation in feedforward nets: \\nSome experiments. In Advances in neural information processing systems (pp. 630 -637). ',\n 'Another practical example is at Tamr.com that offers one of the leading solutions for curating or \\npreparing data at scale, e.g., data from 100,000 typically heterogeneous data sources. It launched initially \\nwith a comprehensive solution  in the domain of information services. Tamr soon found that every new \\ndomain required  a  substantial  revision of  the machine  learning  component.  Initially, like most  AI -based \\nstartups, their initial solution was not generalizable. As can be seen at Tamr.com , Tamr now has solutions \\nin many domains for which they have substantial commonality in the underlying solutions.  \\nAnother  fundamental  difference  between  science  and  data  science  concerns  the scale  and \\nnature  of  the outcomes.  The scientific method is used t o  discover  causal  relationships  between  a  small \\nnumber of variables that represent the essential characteristics of the natural phenomena being analyzed. \\nThe  experimental  hypothesis  defines  the  correlation  to  be  evaluated  for  causality.  The  number  of \\nvariables in a scientific experiment is kept small due to the cost of evaluating a potentially vast number of \\ncombinations  of  variables  of  interest.  PhD  theses,  i.e.,  an  experiment  conducted  by  one  person,  are \\nawarded on experiments with two or three but certai nly less than ten variables. Large -scale experiments, \\ne.g. LIGO13\\n, Kepler 14\\n, and Higgs -Boson, may consider 100s of variables and take years and thousands of \\nscientists  to  evaluate.  Determining  whether  a  correlation  between  variables  is  causal  tends  to  be  an \\nexpensive and slow process.  \\nData science, on the other hand, is used to rapidly discover as many correlations between the \\ndata values as exist in the data set being analyzed, even with very large models (millions of variables) and \\nvast  data  sets.  Depending on  the  analytical  method  used,  the  number  of  variables  in  a  data  science \\nanalysis  can  be  effectively  unlimited, e.g.,  millions,  even  billions,  as can  be  the  number  of  correlations \\nbetween  those  variables,  e.g.,  billions  or  trillions.  Data  science  analytics  are  executed  by  powerful, \\nefficient  algorithms  using  equally  powerful  computing  infrastructure  (CPUs,  networks,  storage).  The \\ncombined power of new algorithms and infrastructure in the 1990’s led to the current efficacy of machine \\nlearning that in turn c ontributed to the emergence of data science.  \\n3.3 The prime benefit of data science is accelerating discovery  \\nData science and empirical science differ dramatically, hence paradigmatically, in the scale of the \\ndata analyzed. Scientific experiments tend to e valuate a small number, e.g., 10s or 100s, of correlations to \\ndetermine if they are causal, and do so over long periods of time, e.g., months or years. In contrast, data \\nscience can identify effectively unlimited numbers of correlations, e.g., millions, bi llions, or more, in short \\ntime  periods, from minutes  to  days. It is  in  this  sense  that  data  science  is  said  to accelerate  discovery. \\nOriginally  developed  in  the  1990’s  for  scientific  discovery,  the  remarkable  results  of  data  science  have \\nresulted in its be ing applied to all endeavors for which adequate data is available.  The prime benefit of \\ndata science is that it is a new paradigm for accelerating discovery , in general.  \\nIdeally, data science is used to accelerate discovery by rapidly reducing a vast searc h space to a \\nsmall  number  of  correlations  that  are  likely  to  be  casual,  as  indicated  by  their  estimated  probability. \\nDepending on the resources available, some number of the probabilistic correlations are selected to be \\nanalyzed  for  causality  by  well-established  (non -data  science)  means  in  the  domain  being  analyzed.  For \\nexample,  data  science  has  been  used  to  accelerate  cancer  drug  discovery.  The  Baylor-Watson  study \\n(Spangler et. al., 2014) used data science methods to identify nine likely cancer drug candid ates. It used a \\nsimple, novel method to further evaluate their likelihood. The original analysis was conducted over drug \\nresearch results published up to 2003 and identified nine likely candidate drugs. The likelihood of those \\nnine candidate drugs was rais ed significantly when the research published from 2003 to 2013 showed that \\nseven of the nine candidates had been validated as genuine cancer drugs. This raised the likelihood that \\nthe remaining two candidate drugs were real. Standard EPA -approved drug deve lopment and clinical trial \\ntesting  were  then  used  to  develop  the  two  new  drugs.  In  this  case,  data  science  accelerated  drug \\ndiscovery for a specific type of cancer. It started with a vast search space of cancer research results from \\n240,000 papers. In thre e months it discovered the two highly likely cancer drug candidates. Conventional \\n                                                               \\n13\\n http://www.ligo.org/  \\n14\\n https://keplerscience.arc.nasa.gov/  ',\n 'cancer drug discovery typically discovers one drug every two to three years. These times do not include \\nthe drug development and clinical trial periods.  \\n3.4 Causal reasoning  in data science is complex and can be dangerous   \\nJust  as  the  scale  is  radically  different  so  is  the  nature  of  the  results.  The  scientific  method \\ndiscovers results that, if executed correctly, are definitive, i.e., true or false, with a defined probability  and \\nerror bound, that a hypothesized relationship is causal. Data science discovers a potentially large number \\nof  correlations  each  qualified  by  a  probability  and  error  bound  that  indicate  the  likelihood  that  the \\ncorrelation may be true.  Data science is used to discover correlations; it is rarely used to determine causal \\nrelationships. The previous sentence is often misunderstood not just by novices, but also, unfortunately, \\nby data scientists. Empirical science discovers causal relationships in one step. D ata science is frequently \\nused to discover causal relationships in two steps: First, discover correlations with a strong likelihood of \\nbeing causal; then use non -data science methods to validate causality. \\nCausality  is  the  Holy  Grail of  science,  scientific discovery,  and  if  feasible,  of  data  science. \\nTypically, the goal of analyzing a phenomenon is to understand  Why some aspects of the phenomenon \\noccur, for example, why does it rain? Prior to a full understanding of the phenomenon, initial discovery is \\noften used to discover  What  conditions prevail when the phenomenon manifests, e.g., as rain starts and \\nduring rain many raised umbrellas can be observed. A more informed observer may also discover specific \\nclimatic conditions. All of the conditions observed to  be  present consistently  before  and  during  the  rain \\ncould  be  said  to  be  correlated  with  rain.  However,  correlation  does  not  imply  causation,  e.g.,  raised \\numbrellas  may  be  correlated  with  rain,  but  do  not  cause  the  rain  (Brodie,  2014a).  A  more  realistic \\nexample  comes  from  an  online  retailer  that  observing  that  increased  sales  were  correlated  with \\ncustomers purchasing with their mobile app, invested significantly to get their app onto many customers’ \\nsmartphones. However, the investment was lost since sales did not increase. Increased purchases were \\ncorrelated with mobile apps on customers’ smartphones; however, the causal factor was customer loyalty \\nand, due to their loyalty, most loyal customers already had the app on their smartphones.   \\nData Science is used  predominantly to discover  What . Empirical science and many other methods \\nare used to discover  Why (Brodie, 2018a). Data science is often used to rapidly reduce the search space \\nfrom  a  vast  number  of  correlations  or  possible  results  to  a  much  smaller  number.  The  much  smaller \\nnumber  of  highly  probable results  are  then  analyzed  with  non -data  science  methods,  such  as  scientific \\nexperiments  or clinical  trials,  to  verify  or  reject  the result,  i.e.,  automatically  generated  hypotheses,  as \\ncausal.   \\nThere are mathem atics and methods claimed for deducing causal effects from observational data \\n(i.e., data not from controlled experiments but from surveys, censuses, administrative records, and other \\ntypically  uncontrolled  sources  such  as  in  Big  Data  and  data  science).  They  are  very  sophisticated  and \\nrequire a deep understanding of the mathematics, statistics, and related modelling methods. Judea Pearl \\nhas  developed  such methods  based  on  statistics, Bayesian  networks,  and related  modelling,  see (Pearl, \\n2009a,b,c). For decades, statisticians and econometricians have developed such methods with which to \\nestimate  causal  effects  from  observational  data,  since  most  social  and  economic  data  is  purely \\nobservational (Winship et. al., 1999).  \\nCausal reasoning involves going beyond th e mathematics and modelling for data science in which \\ncorrelations are obtained. “One of Pearl’s early lessons is that it’s only possible to draw causal conclusions \\nfrom observational (correlational) data if you are willing to make some assumptions about t he way that \\nthe data were sampled and about the absence of certain confounding influences. Thus, my understanding \\nis that one can draw causal conclusions, but it’s important to remember that these are really conditional \\non  the  validity  of  those  assumptions.”  says  Peter  Szolovits,  Professor,  CSAIL,  MIT,  with  a  decade  of \\nexperience applying data science in medical contexts for which he provided an example 15\\n. \\n                                                               \\n15\\n The full quote from personal communication: “There are various soph isticated ways to do all this but let me give you a relatively \\nsimple example: Suppose that we observe that in some cohort of patients, some were treated with drug X and others with drug Y . \\nSuppose further that we see that fewer of the X patients died than  of the Y ones. It’s certainly NOT acceptable to conclude that X is a \\nbetter drug, because we can’t exclude the possibility that the treating doctors’ choice of X or Y depended on some characteristics of ',\n 'Finding correlations between variables in (Big) data together with probabilities or likelihoods of \\nthe correlation  occurring  in  the  past  or  future,  are  relatively  easy  to  understand  and  safe  to  report. \\nMaking  a  causal  statement  can  be  misleading  or  dangerous  depending  on  the  proposed  actions  to  be \\ntaken as a consequence. Hence, I do not condone nor confirm  causal reasoning; it is above my pay grade; \\nhence,  I  quote  experts  on  the  topic  rather  than  make  my  own  assertions.  I  recommend  that  causal \\nreasoning  not  be  applied  without  the  required  depth  of  knowledge  and  experience,  because  making \\ncausal  statements  as  a  result  of  data  science  analysis  could  be  dangerous.  In  lecturing  on  correlation \\nversus causation for over five years, I have found that an inordinate amount of interest is given to this \\ndifficult and little understood topic, perhaps with a desire to be  able to provide definitive answers, even \\nwhen  there  are  none.  I  have  found  no  simple  explanation.  You  either  study,  understand,  and  practice \\ncausal  reasoning  with  the  appropriate  care  or  simply  stay  away  until  you  are  prepared.  Experts  are \\nappropriately ca utious. “I have not, so far, made causal claims based on my work, mainly because I have \\nnot felt strongly enough that I could defend the independence assumptions needed to make such claims. \\nHowever,  I  think  the  kinds  of  associational  results  are  still  possibly  helpful  for  decision  makers  when \\ncombined  with  intuition  and  understanding.  Nevertheless,  I  think  most  clinicians  today  do  not  use \\npredictive models other than for more administrative tasks such as staffing or predicting bed occupancy” \\n– Peter Szolovi ts, MIT. “I firmly believe that [deriving] causal results from observational data is one of the \\ngrand  challenges  of  the  data  science  agenda!” – David  Parkes,  co-lead  of  the  Harvard  Data  Science \\nInitiative.  \"Pearl once explained those ideas to me personally  at Santa Catalina workshop, but I still don’t \\nfully understand them either :)”  – Gregory Piatetsky-Shapiro, President of KDnuggets, co -founder of KDD \\nConferences and ACM SIGKDD.  \\n3.5 Data science flexibility: data -driven or hypothesis -driven \\nEmpirical science and data science have another fundamental difference. The scientific method \\nuses  deductive  reasoning,  also  called  hypothesis-driven,  theory-driven,  and  top-down.  Deductive \\nreasoning is used when specific hypotheses are to be evaluated aga inst observations or data . A scientific \\nexperiment starts by formulating a hypothesis to be evaluated. An experiment is designed and executed, \\nand the results interpreted to determine if the hypothesis is true or false under the conditions defined for \\nthe hypothesis. It is called theory -driven in that a theory is developed, expressed as a hypothesis, and an \\nexperiment  designed  to  prove  or  invalidate  the  hypothesis.  It  is  called  top -down  since  the  experiment \\nstarts at the top  – with the idea – and goes down  to the data to determine if the idea is true.  \\nData science can be hypothesis -driven.  That is, as with empirical science, a data science activity \\ncan start with a hypothesis to be evaluated. Unlike empirical science, the hypothesis can be stated with \\nless precision and the models, methods, and data can be much larger in scale, i.e., more va riables, data \\nvolume,  velocity,  and  variety.  In  comparison,  data  science  accelerates  discovery  by  rapidly  reducing  a \\nvastly larger search space than would have been considered for empirical methods, to a small set of likely \\ncorrelations; however, unlike em pirical science, the results are correlations that require additional, non -\\ndata science methods to achieve definitive, causal results.  \\nOne of the greatest advantages of data science is that it can discover patterns or correlations in \\ndata  at  scale  vastly  beyond  human  intellectual,  let  alone  temporal,  capacity;  far  beyond  what  humans \\n                                                                                                                                                                                          \\nthe patient that also influenced their likelihood of  survival. E.g., maybe the people who got Y were much sicker to start with, because \\nY is a stronger and more dangerous drug, so it is only given to the sickest patients.  \\nOne way to try to mitigate this is to build a model from all the data we have about th e patients in the cohort that predicts \\nwhether they are likely to get X or Y. Then we stratify the cohort by the probability of getting X, say. This is called a  propensity score . \\nAmong those people with a high score, most will probably actually get X (that ’s how we built the model), but some will nevertheless \\nget Y, and  vice versa. If we assume that the doctors choosing the drug have no more information than the propensity model, then \\nwe treat their choice to give X  or Y as a random choice, and we analyze  the resulting data as if, for  each stratum, patients  were \\nrandomized  into  get ting  either  X  or  Y,  as  they  might  have  been  in  a  real  clinical  trial.  Then  we  analyze  the  results  under  that \\nassumption.  For  many  of  the  strata  where  the  propensity  is  not  near  .5,  the  drugs  given  will  be  unbalanced,  which  makes  the \\nstatistical power of th e analysis lower, but there are statistical methods for dealing with this. Of course, the conclusions one draws \\nare still very much dependent of the assumption that, within each stratum, the doctors’ choice of drug really is random, and  not a \\nfunction of s ome difference among the patients that was not captured in the data from which the propensity model was built. \\nThis is just one of numerous methods people have invented, but it is typical of the kinds of assumptions one has to make \\nin order to draw causal  conclusions from data.”  ',\n 'could have conceived. Of course, a vast subset of those found may be entirely spurious. Data science can \\nuse  inductive  reasoning,  also  called  bottom-up,  data-driven,  or  fact-based  analysis,  not  to  evaluate \\nspecific  hypotheses  but  using  an  analytical model  and method  to  identify  patterns  or correlations  that \\noccur in  the  data  with  a  specific frequency. If  the frequency  meets  some  predefined  specification,  e.g., \\nstatistical significance in the domain being analyzed, it can be interpreted as a measure of likelihood of \\nthe pattern being real. As opposed to evaluating pre -defined hypotheses in the theory -driven approach, \\nthe data-driven approach is often said to “automatically” generat e  hypotheses,  as  in  (Nagarajan,  2015). \\nThe inductive capacity of data science is often touted as its magic as the machine or methods such as \\nmachine  learning,  “automatically”  and  efficiently  discover  likely  hypotheses  from  the  data.  While  the \\nacceleration  and the scale of data being analyzed are major breakthroughs in discovery, the magic should \\nbe moderated by the fact that the discovered hypotheses are derived from the models and methods used \\nto discover them. The appearance of magic may derive from the f act that we may not understand how \\nsome  analytical methods,  e.g.,  some machine  learning  and  deep  learning methods,  derive their  results. \\nThis is a fundamental data science research challenge as we would like to understand the reasoning that \\nled to a discov ery, as is required in medicine, and in 2018 in the European Union, by law (the General Data \\nProtection Regulation (GDPR 16\\n)). \\n3.6 Data science is in its infancy  \\n   The excitement around data science and its many successes are wonderful, and the potential of \\ndata science is great, but these positive signs can be misleading. Not only is data science in its infancy as a \\nscience and a discipline, its current practice has a large learning curve related largely to the issues raised \\nabove. Gartner, Forrester, and ot her technology analysts report that most (80%) early (2010 -2012) data \\nscience projects in most US enterprises failed. In late 2016, Gartner reported that while most enterprises \\ndeclare  data  science  as  a  core  expertise,  only  15%  claim  to  have  deployed  big  data  projects  in  their \\norganization  (Gartner,  2016).  Analysts  predict  80+%  failure  rate  through  2017  (Demirkan  &  Dal,  2014) \\n(Veeramachaneni , K. 2016) (Lohr & Singer, 2016).  \\n3.7 It’s more c omplicated than that  \\nData  science  methods  are  more  sophisticated  than  the  above  descriptions  suggest,  and  data -\\ndriven analyses are not as pure. Data science analytical methods and models do not discover any and all \\ncorrelations that exist in the data since  they are discovered using algorithms and models that incorporate \\nsome hypotheses that could be considered biases. That is, you discover what the models and methods are \\ndesigned to discover. One must be objective in data science across the entire workflow  - data selection, \\npreparation, modelling, analysis, and interpretation; hence, a data scientist must always Doubt and Verify \\n(Brodie, 2015b).  \\nIt  may  be  useful  to  experiment  with  the  models  and  methods.  When  a  data  science  analysis \\nreduces  a  vast  search  space,  it  (or  the  observing  human)  may  learn  something  about  the  discovered \\ncorrelations and may warrant an adjustment and re -running the model, the method, or even adjusting the \\ndata  set.  Hence,  iterative  learning  cycles  may  increase  the  efficacy  of  the  analysis  or  simply  provide  a \\nmeans of exploring the data science analysis search space.   \\nTop-down and bottom -up analytical methods can be used in combination, as follows. Start with a \\nbottom -up analysis that produces N candidate correlations. Select a subset o f K of the correlations with \\nan acceptable likelihood and treat them as hypotheses to be evaluated. Then use them to run hypothesis -\\ndriven data science analyses and determine, based on the results, which hypotheses are again the most \\nlikely or perhaps even  more likely than the previous run and discard the rest. These results can be used in \\nturn  to  redesign  the  data  science  analysis,  e.g.,  iteratively  modify  the  data,  model,  and  method,  and \\nrepeat the cycle. This approach is used to explore data, models, and  methods  - the main components of a \\ndata science activity. This method of combining top -down and bottom -up analysis has been proposed by \\n                                                               \\n16\\n https://en.wikipedia.org/wiki/General_Data_Protection_Regulation  ',\n \"CancerCommons, as a method for accelerating the development of cancer cures as part of the emerging \\nfield of translatio nal medicine. 17\\n  \\n4 Data science components  \\nExtending the analogy with science and the scientific method, data science, when mature, will be a \\nsystematic discipline with components that are applicable to most domains  – to most human endeavors. \\nThere are four c ategories of data science components, all emergent in the data science context awaiting \\nresearch and development: 1) principles, data, models, and methods; 2) data science pipelines; 3) data \\nscience infrastructure; and 4) data infrastructure. Below, we discuss these components in terms of their \\nsupport of a specific data science activity.  \\nSuccessful data science activities have developed and deployed these components specific to their \\ndomain and analysis. To be considered a science, these components must be  generalized across multiple \\ndomains, just as the scientific method applies to most scientific domains, and in the last century has been \\napplied  to  domains  previously  not  considered  scientific,  e.g.,  economics,  humanities,  literature, \\npsychology, sociology , and history.  \\n4.1 Data science principles, data, models, and methods   \\nA data science activity must be based on  data science principles , models , and analytical  methods. \\nPrinciples  include  those  of  science  and  of  the  scientific  method  applied  to  data  science,  for  example, \\ndeductive and inductive reasoning, objectivity or lack of bias relative to a given factor, reproducibility, and \\nprovenance.  Particularly  important  are  collaborative  and  cross -disciplinary  methods.  How  do  scientific \\nprinciples apply to discovery over data? What principles underlie evidence -based reasoning for planning, \\npredicting, decision -making, and policy -making in  a specific domain?  \\nIn May 2017, the  Economist declared, on its front cover, that data  was The World’s Most Valuable \\nResource  (Economist, May 2017). Without data there would be no data science or any of its benefits. Data \\nmanagement has been a cornerstone of computer science technology, educat ion, and research for over \\n50 years, yet Big Data that is fueling data science, is typically defined as data at volumes, velocities, and \\nvariety  that  cannot  be  handled  by  data  management  technology.  A  simple  example  is  that  data \\nmanagement functions in pre paring data for data analysis take 80% of the resources and time for most \\ndata science activities. Data management research is in the process of flipping that ratio so that 80% of \\nresources can be devoted to analysis. Discovering data required for a data s cience activity whether inside \\nor outside an organization is far worse. Fundamental data research is required in each step of the data \\nscience pipeline to realize the benefits of data science.  \\nA data science activity uses one or more models. A model repres ents the parameters that are the \\ncritical  properties  of  the  phenomenon  to  be  analyzed.  It  often  takes  multiple  models  to  capture  all \\nrelevant  features.  For  example,  the  LIGO  experiment,  that  won  the  2017  Nobel  Prize  in  Physics for \\nempirically establishing  the existence of Einstein's gravitational waves, had to distinguish movement from \\ngravitational waves from seismic activity and 100,000 other types of movement. LIGO required a model \\nfor  each  movement  type  so  as  to  recognize  it  in  the  data  and  discard  it  as  gravitational  wave  activity. \\nModels are typically domain specific, e.g., seismic versus sonic, and are often already established in the \\ndomain. Increasingly, models are developed specifically for a data science activity, e.g., feature extraction \\nfrom  a  data  set  is  common  for  many  AI  methods.  Data  science  activities  often  require  the  continuous \\nrefinement  of  a  model  to  meet  the  analytical  requirements  of  the  activity.  This  leads  to  the  need  for \\nmodel management to capture the settings and results of the pl anned and evaluated model variations. It \\nis increasingly common, as in biology, to use multiple, distinct models, called an ensemble of models, each \\nof which provides insights from a particular perspective. Each model, like each person in Plato’s Allegory \\nof  the  Cave,  represents  a  different  perspective  of  the  same  phenomenon,  what  Plato  called  shadows. \\nEach  model  – each  person – observes  what  appears  to  be  the  same  phenomenon,  yet  each  sees  it \\ndifferently.  No  one  model – person  – sees  the  entire  thing,  yet collectively  they  capture  the  whole \\nphenomenon  from  many  perspectives.  It  may  also  be  that  a  critical  perspective  is  missed.  It  is  rarely \\n                                                               \\n17\\n The National Center for Advancing Translational Sciences  https://ncats.nih.gov   \",\n 'necessary, feasible, or of value to integrate different perspectives into a single integrated model. After all, \\nthere is no ultimate or truthful model save the phenomenon itself. Ensemble or shadow modelling is a \\nnatural and nuanced form of data integratio n (Liu, 2012)  analogous to ensemble modelling in biology and \\nensemble learning ( Dietterich, 2000) and forecasting in other domains.  \\nA  data  science  activity  can  involve  many  analytical  methods.  A  given  method  or  algorithm  is \\ndesigned to analyze specific features of a data set. There are often variations of a method depending on \\nthe characteristics of th e data set, e.g., sparse or dense, uniform or skewed, data type, data volume, etc., \\nhence methods must be selected, or created, and tuned for the data set and analytical requirements, and \\nvalidated.  In  an  analysis,  there could  be  as  many  methods  as  there  are  specific  features  with \\ncorresponding  specific  data  set  types.  Compared  with  analytical  methods  in  science,  their  definition, \\nselection,  tuning,  and  validation  in  data  science  often  involves  scale  in  choice  and  computational \\nrequirements. Unless they are  experts in the related methods, it is unlikely that a practicing data scientist \\nunderstands  the  analytical  method,  e.g.,  a  specific  machine  learning  approach,  that  they  are  applying \\nrelative  to  the  analysis  and  data  characteristics,  let  alone  the  thousands  of  available  alternatives. \\nAnecdotally, I have found that many practicing data scientists use the algorithms that they were taught \\nrather than selecting the one most applicable to the analysis at hand.  There are significant challenges in \\napplying  sophisticated  analytical models  and  methods  in  business (Forrester,  2015) .  Having  selected  or \\ncreated  and  refined  the appropriate  model,  i.e.,  collection  of  features  that determine  the  data  to  be \\ncollected, collected and prepared the data to comply with the requi rements of  the model,  and  selected \\nand refined the appr opriate analytical method, the next challenge is interpreting the results  and, based on \\nthe data, model, and method, evaluate  the likelihood, within relevant error bounds, that the results are \\nmeaningf ul hypotheses worthy of validating by other means. \\n4.2 Data science workflows or pipelines  \\nThe central  organizing  principle  of  a  data  science  activity is its  workflow  or pipeline  and  its  life \\ncycle  management  (NSF,  2016) .  A  data  science  pipeline  is  an  end-to-end  sequence  of  steps  from  data \\ndiscovery to the publication of the qualified, probabilistic interpretation of the result in the form of a data \\nproduct.  A  generic  data  science  pipeline,  such  as  listed  below,  is  comprehensive  of  all  data  science \\nactivities, hence can be used to define the  scope of data science . \\n  \\n1. Raw data discovery, acquisition, preparation, and storage as curated data in data repositories   \\n2. Selection and acquisition of curated data from data repositories for data analysis   \\n3. Data analysis  \\n4. Results interpretation  \\n5. Result publication and optionally operationalize the pipeline for continuous analyses  \\n \\nThe  state  of  the  art  of  data  science  is  such  that  every  data  science  activity  has its  own  unique \\npipeline,  as  each  data  science  activity  is  unique. Due  to  the  emergence  and broad  applicability  of  data \\nscience, there  is  far  more  variation across data science pipelines  than  across conventional  science  \\npipelines . Data science will benefit , as it develops,  from a better understanding of pipelines and guidance \\non their design and development.  \\nData  science  pipelines  are  often  considered  only  in  terms of  the analytics,  e.g.,  the  machine \\nlearning  algorithms  used  to  derive  the  results  in  step  3.  However,  most  of  the  resources  required  to \\ndesign, tune, and exe cute a data science activity are  required  not for data analysis, steps 3 and 4  of a data \\nscience pipeline,  but for the design and development of the pipeline and for steps 1 and 2.  \\nThe design, development, and tuning of an end -to-end  pipeline for  a  data  science  activity  typically \\nposes  significant  data  modelling,  preparation,  and  management  challenges  often  requiring  significant \\nresources  and  time  required  to  develop  and  execute  a  data  science  activity.  Two  examples  are \\nastrophysical  experiments,  the  Kepler  Space  Telescope  launched  in  2009  to  find  exoplanets  and  Laser \\nInterferometer Gravitational -Wave Observatory (LIGO) that was awarded the 2017 Nobel Prize in Physics. \\nInitial versions of the experiments failed not because of analysis and astrophysical aspec ts and models, \\nbut due to the data pipelines. Due to unanticipated issues with the data, the Kepler Science Pipeline had ',\n 'to  be  rewritten  (Jenkins,  2010) while  Kepler  was  inflight  retaining  all  data  for  subsequent  corrected \\nprocessing.  Similarly,  earth -based  LIGO’s  pipeline  was  rewritten  (Singh,  2007)  and  renamed  Advanced \\nLIGO. Tuning or replacing the faulty pipelines delayed both experiments by approximately one year.  \\nOnce the data has been acquired, the most time -consuming activity in developing a pipeline  was data \\npreparation.  Early  data  science  activities  in  2003  reported  80-90%  of  resources  devoted  to  data \\npreparation  (Dasu & Johnson, 2003). By 2014 this was reduced to 50-80% (Lohr, 2014). In specific cases, \\nthis cost negatively impacted some domains (Re imsbach -Kounatze,  2015)  due  to  the massive  growth  of \\nacquired  data.  As  data  science  blossomed  so  did  data volumes,  leading  experts  in  2015  to  analyze  the \\nstate  of  the art  and  estimating  that  data  preparation  typically  consumed  80%  of resources  (Castanedo, \\n2015). By then products to curate data at scale, such as Tamr.com, were maturing and being more widely \\nadopted. Due to the visibility of data science, the popular press surveyed data scientists to confirm the \\n80%  estimates  (Press,  2016;  Thakur  2016).  In  2017,  technical  evaluations  of  data  preparation  products \\nand their use again identified the 2003 estimates of 80% (Mayo, 2017) (Gartner G00315888, 2017) . \\n4.3 Data science and data infrastructures  \\nThe  core  technical  component  for  a  data  science  activity  is  a data  science  infrastructure that \\nsupports  the  steps  of  the  data  science  pipeline  throughout  its  life  cycle.  A  data  science  infrastructure \\nconsists of a workflow platform that supports the definition, refinement, execution, and reporting of data \\nscience  activities  in  the  pipeline. The  workflow  platform  is  supported by  the infrastructure  required  to \\nsupport  workflow  tasks  such  as  data  discovery,  data  mining,  data  preparation,  data  management, \\nnetworking,  libraries  of  analytical  models  and  analytical  methods, visualization,  etc.  To  support  user \\nproductivity, a user interface is required for each class of user, each with their own user experience. There \\nare  more  than  60  such  data  science  platforms - a  new  class  of  product - of  which  16  meet analysts’  \\nrequirement s (Gartner G00301536, 2017) (Gartner G00326671, 2017) (Forrester, 2017) . These products \\nare  complex  with  over  15  component  products  such  as  database  management,  model  management, \\nmachine  learning,  advanced  analytics,  data  exploration,  visualization,  and  da ta  preparation.  The  large \\nnumber of products reflects the desire to get into a  potentially  large, emerging market ; regardless of their \\ncurrent ability to support data science 18\\n. \\nData, the world’s most valuable resource  (Economist, May 2017) , is also the most valuable resource \\nfor the data science activities of an organization (e.g., commercial, educational, research, governmental) \\nand  for  entire  communities.  While  new  data  is  always  required  for  an  existing  or  new  data  science \\nactivity,  data science  activities  of  an  organization  require  a data  infrastructure  – a  sustainable,  robust \\ndata  infrastructure  consisting  of  repositories  of  raw  and  curated  data  required  to  support  the  data \\nrequirements of the organization’s data science activities with  the associated support processes such as \\ndata stewardship. Many organizations are just developing  data infrastructures for  data science , aka data \\nscience platforms.  The best  known  are  those  that  support large research  communities. The  US  National \\nResearch  Foundation is developing the  Sustainable Digital Data Preservation and Access Network Partners \\nto support data science for national science and engineering research and education. The 1000 Genomes \\nProject Consortium created the world’s largest catalog of  genomic differences among humans, providing \\nresearchers  worldwide with  powerful  clues  to  help  them establish  why  some  people  are  susceptible  to \\nvarious diseases. There are more than ten additional genomics data infrastructures, including the Cancer \\nGenome  Atlas of the US National Institutes of Health, Intel’s Collaborative Cancer Cloud, and the Seven \\nBridges  Cancer  Cloud. Amazon  hosts19\\n the  1000  Genome  Project  and  30  other  public  data \\ninfrastructures  on  topics  such  as geospatial  and environmental  datasets,  genomics  and life science \\ndatasets, and  datasets for  machine  learning. The Swiss Data Science Center started developing the Ren ga \\nplatform 20\\n to support data scientists with their complete workflow.  \\n                                                               \\n18\\n By  1983  in  response  to  the  then  emerging  technology  of  relational  database  management  systems \\n(DBMSs) there  were over 100 Relational DBMSs of which five survived.  \\n19\\n https://aws.amazon.com/public -datasets/  \\n20\\n https://datascience.ch/renga -platform/  ',\n '5 What is the method for conducting data science?  \\nA  data  science  activity  is  developed  based  on  data  science  principles,  models  and  analytical \\nmethods. The result of its design and development is a data science pipeline that will operate on a data \\nscience infrastructure,  or  platform,  and  will  access  data in  a  data  infrastructure.  There  are  a  myriad  of \\ndesign and development methods to get from the principles to the pipeline. What follows is a description \\nof a fairly generic data science method.   \\nThe  data science method ,  until better  alternatives  arise, is modelled on the scientific method. The \\nfollowing  is  one  example  of  applying  the  empirical  approach  to  data  science  analysis,  analogous  to \\nexperimental  design  for  science  experiments.  Each  step  requires  verification,  e.g.,  using  experts, \\npublished literatur e, previous analysis; and continuous iterative improvement to reach results that meet a \\npredefined specification. Each step may require revisiting a previous step, depending on its outcome. As \\nwith  any  scientific  analysis,  every  attempt  should  be  made  to  avoid  bias,  namely,  attempting  to  prove \\npreconceived ideas beyond the model, methods, and hypotheses. The method may run for hours to days \\nfor a small analysis; months, as for the Baylor -Watson drug discovery (Spangler et. al., 2014); or years, as \\nfor the K epler Space Telescope and LIGO. Design and development times can be similar to run times. Otto \\nfor example, a German e -commerce merchant, developed over months an AI-based system that predicts \\nwith  90%  accuracy  what  products  will  be  sold  in  the  next  30  days  and  a  companion  system  that \\nautomatically  purchases  over  200,000  products21\\n a  month  from  third-party  brands  without  human \\nintervention.  Otto  selected,  modified,  and  tuned  a  deep-learning  algorithm  originally  designed  for \\nparticle -physics experiments at CE RN (Economist, April 2017). These systems run continuously.  \\n5.1 A Generic Data Science Method 22\\n \\n1. Identify the phenomena or problem to be investigated. What is the desired outcome?  \\n2. Using  domain  knowledge,  define  the  problem  in  terms  of features  that  represent the  critical \\nfactors  or  parameters  to  be  analyzed  (the  WHAT  of  your  analysis,  that collectively  form  the \\nmodel),  based  on  the  data  likely  to  be  available  for  the  analysis.  Understanding  the  domain \\nprecedes defining hypotheses to avoid bias.  \\n3. If the analysi s is to be top-down, formulate the hypotheses to be evaluated over the parameters \\nand models.  \\n4. Design the analysis in terms of an end -to-end workflow or pipeline from the data discovery and \\nacquisition,  through  analysis  and  results  interpretation.  The  analysis  should  be  designed  to \\nidentify  probabilistically  significant  correlations  ( What)  and  set  requirements  for  acceptable \\nlikelihoods and error bounds.  \\n5. Ensure the conceptual validity of the data analysis design.  \\n6. Design, test, and evaluate each step in the p ipeline, selecting the relevant methods, i.e., class of \\nrelevant algorithms, in preparation for developing the following steps.   \\na. Discover, acquire, and prepare data required for the parameters and models ensuring \\nthat the results are consistent with previous steps.  \\nb. For  each  analytical  method,  select  and  tune  the  relevant  algorithm  to  meet  the \\nanalytical  requirements.  This  and  the  previous  step  are  highly  interrelated  and  often \\nexecuted iteratively until the requirements are met with test or training data.  \\nc. Ensure the validity of the data analysis implementation.  \\n7. Execute the pipeline ensuring that requirements, e.g., probabilities and  error bounds, are met.  \\n8. Ensure  empirical  (common  sense)  validation  - the  validity  of  the  results  with  respect  to  the \\nphenomena being investigated.  \\n                                                               \\n21\\n Stock Keeping Units (SKUs).  \\n22\\n This set of steps was derived from analyzing over 150 data science activities. It’s purpose is as a basis for \\nguidance for those new to data science and as one alternative to data scientists looking for commonality \\nacross domains.  ',\n '9. Interpret  the  results  with  respect  to  the  models,  methods,  and  data  analytic  requirements. \\nEvaluate  the  result s  (patterns  or  correlations)  that  meet  the  requirements  for  causality  to  be \\nvalidated by methods outside data science.  \\n10. If the pipeline is to operate continuously, operationalize and monitor the pipeline and  its results . \\n6 What is data science in practice?  \\nEach data science activity develops its own unique data science method. Three very successful data \\nscience activities are described below in point form descriptions, using the above terminology to illustrate \\nthe components of data science in practice. They w ere conducted over 18, 20, and 2 years respectively. \\nTheir data science pipelines operated for 4 years, 3 years (to date), and 3 months respectively.  \\n6.1 Kepler Space Telescope: Discovering Exoplanets   \\nThe  Kepler  Space Telescope,  initiated  in  1999,  and  its  successor  project  K2,  have  catalogued \\nthousands of exoplanets by means of data analytics over Big Data. A detailed description of Kepler and \\naccess to its data is at  NASA’s Kepler & K2  Website 23\\n. \\n● Objective and phenomenon : Discover exoplanets in telescopic images  \\n● Project: NASA-led collaboration of US government agencies, universities, and companies.  \\n● Critical parameters : Over 100, e.g., planet luminosity, temperature, planet location relative to its \\nsun. \\n● Models :  There  are  over  30  established  astrophysical  models.  A  key Kepler  model  is  the \\nrelationship  between  luminosity , size,  and  temperature. This  model was  established  a  century \\nago  by  Ejnar  Hertzsprung  and  Henry Russell.  This illustrates  the  fact  that  data  science involves  \\nmany models and analytical methods that have nothing to do with AI.  \\n● Methods :  Over  100,  e.g.,  multi-scale  Bayesian  Maximum  A  Priori  method  used  for  systematic \\nerror removal from raw data. AI was not a principle method in this project.  \\n● Hypotheses  (stated in  Kepler documents as a query): F ive, including “Determine the percentage \\nof terrestrial and larger planets that are in or near the habitable zone of a wide variety of stars”.  \\n● Data: 100’s of data types described in the Data Characteristics Handbook 24\\n in the  NASA \\nExoplanet Archive 25\\n \\n● Pipeline:  The Kepler  Science  Pipeline26\\n failed  almost  immediately  after  launch  due  to \\ntemperature and other unanticipated issues. After being repaired from earth, it worked well for \\n4 years. \\n● Data discovery and acquisition : Required approximately 90% of the total effort and resources. \\n● Algorithm  selecting  and  tuning:  Models  and  methods  were  selected,  developed,  tuned  and \\ntested for the decade from project inception in 1999 to satellite launch in 2009, and were refined \\ncontin uously.  \\n● Verification:  Every model and method  were verified, e.g., exoplanet observations were verified \\nusing the Keck observatory in Hawaii.  \\n● Probabilistic outcomes 27\\n \\nKepler:  \\n● Candidates (<95%): 4,496  \\n● Confirmed (>99%): 2,330  \\n● Confirmed: <2X Earth -size in habitable zone: 30  \\n● Probably (<99%): 1,285  \\n● Probably not (~99%): 707  \\nK2: \\n                                                               \\n23\\n https://keplerscience.arc.nasa.gov/  \\n24\\n https://archive.stsci.edu/kepler/manuals/Data_Characteristics.pdf  \\n25\\n https://exoplanetarc hive.ipac.caltech.edu/docs/KeplerMission.html  \\n26\\n https://keplerscience.arc.nasa.gov/pipeline.html  \\n27\\n Kepler’s data is available at  http://exoplanetarchive.ipac.calte ch.edu   ',\n ' ● Candidate (<95%): 521  \\n● Confirmed (>99%): 140  \\n6.2 LIGO: Detecting Gravitational Waves  \\nThe  LIGO  project  detected  cosmic  gravitational  waves  predicted  by  Einstein’s  1916  Theory  of \\nGeneral Relativity for which it’s originators were awarded the 2017 Nobel Prize. Project information and \\nits data are available at the LIGO Scientific Collaboration website 28\\n. \\n● Objective and phenomenon : Observe cosmic gravitational waves.  \\n● Project: Initiated in 1997 with 1,000 scientists in 100 institutes across 18 countries. \\n● Equipment :  Laser  Interferometer  Gravitational-Wave  Observatory  (world’s  most  sensitive \\ndetector).  \\n● Go Live:  September 2015 (after a  massive upgrade).  \\n● Data:  100,000 channels of measurement of which one is for gravitational waves.  \\n● Models : At least one model per channel.  \\n● Methods :  At  least  one  data  analysis method  per  data  type  being  analyzed.  Initially, AI was not \\nused. In the past two y ears Machine Learning has been found to be very effective in many areas, \\ne.g., detector malfunctions, earthquake detection.   \\n● Challenges:  Equipment and pipeline (as is typical in data science activities).  \\n● Results:  \\no In September  2015  (moments  after  reboot  following  the  massive  upgrade),  a \\ngravitational wave, ripples in the fabric of space -time, was detected and estimated to be \\nthe result of two black holes colliding 1.3BN light years from Earth.  \\no Since then, four more gravitational waves were detected, one as this chapter went to \\npress. \\n● Collaboration : The project depended on continuous collaboration between experimentalists who \\ndeveloped the equipment and theorists who defined what a signal from two black holes colliding \\nwould look like, let alone collaboration scientists, institutes, and countries.  \\n6.3 Baylor -Watson: Cancer Drug Discovery  \\nThe Baylor -Watson drug discovery project  (Spangler et. al., 2014)  is a wonderful example of data -\\ndriven  discovery  and  automatic  hypothesis  generation  that  discovered  two  novel  kinases  as  potential \\nsources for cancer drug development. These results that were determined to have a very high likelihood \\nof  success  were  developed  in  three  months  using  IBM’s  Watson  compared  with  the  typical  multi-year \\nefforts that typically discover one candidate in two years.  \\n● Objective  and  phenomenon:  Discover  kinases  that  regulate  protein  p53  to  reduce  or  stem \\ncancerous cell growth that have  not yet been evaluated as a potential cancer drug.  \\n● Project: Two years starting in 2012 between IBM Watson and the Baylor College of Medicine. \\n● Equipment : Watson as a data science platform; PubMed as data  repository  containing a corpus \\nof 23M medical researc h articles. \\n● Data:  23M abstracts reduced to 240,00 papers on kinases reduced to 70,000 papers on kinases \\nthat regulate protein p53.  \\n● Hypothesis :  Some  of  500  kinases  in  the  corpus  regulate  p53  and  have  not  yet  been used  for \\ndrugs. \\n● AI  Models  /  methods: network  analysis  (Nagarajan,  2015)  including  textual  analysis,  graphical \\nmodels of proteins and kinases, and similarity analysis.   \\n● Pipeline: Explore, Interpret, and Analyze  \\no Explore:  Scan abstracts to select kinase papers using text signatures.   \\no Interpret:  Extract  kinase  entities  from  papers  and build  connected  graph  of  similarity \\namongst kinases.  \\n                                                               \\n28\\n http://www.ligo.org/  ',\n ' o Analyze: Diffuse annotations over kinases to rank order the best candidates for further \\nexperimentation.  \\n● Data discovery and acquisition:  Textual analysis of PubMed.  \\n● Challenge:  Designing, developing and tuning  models and methods to scan abstracts for relevant \\npapers;  to  construct  a  graphical  model  of  the  relevant  relationships,  to  select  kinases  that \\nregulate p53.  \\n● Execution:  3 months. \\n● Results : Two potential cancer drugs in 3 months versus 1 every 2 years (acceleration).  \\n● Validation : The methods discovered 9 kinases of interest analy zing the corpus up to 2003; 7 of 9 \\nwere empirically verified in the period 200 3-2013. This raised the probability that the  remaining \\ntwo that had not yet been verified clinically, were highly likely candidates. \\n● Causality : Work is underway to develop drugs that use the kinases to regulate p53 to stem or \\nreduce cancerous cell growth.  \\n● Collaboration :  The  project  involved  collaboration  between  genetic  researchers,  oncologists, \\nexperts in AI and natural language understanding, and computer scientists.  \\n7 How important is collaboration in data science?   \\nData  science  is  an  inherently  multidisciplinary  activity,  just  as  most  human  endeavors  require \\nknowled ge,  expertise,  methods,  and  tools  from  multiple  disciplines.  Analyzing  real  world  phenomena \\nrequires multidisciplinary approaches, e.g., how can you analyze the politics of a significant event without \\nconsidering  the  economic  factors  (Brodie,  2015c)?  Data science  requires  expertise  from  multiple \\ndisciplines, from  the  subject  domain,  statistics,  AI, analytics, mathematics,  computing, and many more. \\nHowever,  multidisciplinary  collaboration  is  especially  critical  for  success  at  this  early  time  in  the \\nemergence  of  data  science.  Success  and  advancement  in  research  and  industry  are  typically  based  on \\ncompetitive achievements of individual people or teams rather than on collaboration. While collaboration \\nand multidisciplinary thinking  are  praised,  they  are  seldom  t aught  or  practiced.  Successful  data  science \\nrequires a behaviour change from competition to collaboration.  \\nFor  disciplines  required by  scientific  activities,  there  are  well-established  principles, methods,  and \\ntools from each discipline as well as how they  are applied across scientific workflows. Collaboration was \\nbuilt  into  these  mature  disciplines  and  workflows  years  ago.  In  contrast,  the  principles,  methods,  and \\ntools  for  each  relevant  discipline  are  just  emerging  for  data  science,  as  are  methods  of  collaboration \\nacross workflows.  \\nCurrently, data science requires a data scientist to know the sources, conditions, and nature of the \\ndata  to  ensure that  the  domain  specific  model  has  the  appropriate  data. Rather  than  becoming a  data \\nexpert  the  data  scientist  collaborates  with  a  data  expert.  Rather  than  becoming  an  AI  expert,  a  data \\nscientist  may  need  to  collaborate  with  an  AI  expert  to  ensure  the  appropriate  analytical  methods  are \\nused. There can be as many as ten 29\\n disciplines involved in such an activity. Two current challenges in this \\nregard are: 1) the shortage of data science -savvy experts, and 2) moving from a world of individual work \\nto  one  of  collaboration.  Both  challenges  are  being  addressed  by  universities  and  institutes  worldwide; \\nhowever, the knowledg e, as discussed above, and the teachers are themselves new to this game.  \\nThe need for collaboration on basic research and engineering on the fundamental building blocks of \\ndata science and data science infrastructures can be seen in a recent report from Un iversity of California, \\nBerkeley researchers (Stoica, 2017). The report is a collaborative effort from experts from many domains \\n– statistics, AI, data management, systems, security, data centers, distributed computing, and more.   \\nData science activities have emerged in most research labs in most universities and national research \\nlabs. Until 2017, many Harvard University departments had one or more groups conducting data science \\nresearch and offered a myriad of data science degrees  and certificates. In March 2017, the  Harvard Data \\n                                                               \\n29\\n 10  is  a  somewhat  arbitrary  number chosen  because  most  pipelines  involve  5  to  10 expert  tasks. The \\nactual number of required disciplines varies significantly from simple analyses (e.g., reordering products \\nfor an online retailer) to very sophisticated (e.g ., LIGO required analysis of ~1,000 sources of motion).   ',\n 'Science initiative30\\n was established to coordinate the many activities. This pattern has repeated at over \\n120 major universities worldwide, resulting in over 1 50 DSRIs 31\\n being established since 2015  – themselves \\njust  emerging.  The  creation  of  over  150  DSRIs  in  approximately  two  years, most  heavily  funded  by \\ngovernments and by partner industrial organizations, is an indication of the belief in the potential of data \\nscience not just as a new discovery paradigm, but as a basis for business and economic growth.  \\nCollaboration is an emerging challenge in data science not only at the scientific level but also at the \\nstrategic and organizational levels. Analysts report that most early industry  big  data deployments  failed \\ndue to a lack of domain -business -analytics -IT collaboration (Forrester, 2015) . Most of the over 1 50 DSRIs \\ninvolve a grouping of departments or groups with an interest in data science, each in their own dom ain, \\ninto  a  higher  level  DSRI.  A  large  example  is  the Fraunhofer  Big  Data  Alliance32\\n,  which  in  the  above \\nterminology would be a DSRI of DSRIs , describes itself as:  “The Fraunhofer Big Data Alliance consists of 30 \\ninstitutes bundling their cross -sector competencies. T heir expertise ranges from market -oriented big data \\nsolutions for individual problems to the professional education of data scientists and big data specialists.”  \\nIn principle, a DSRI would strive for higher -level, scientific and strategic goals, such as co ntributing to \\ndata  science  (i.e.,  the  science  underlying  data  science)  in  contrast  with  the  contributions  made  in  a \\nspecific  domain  by  each  partner  organization.  But  how  does  the  DSRI  operate?  How  should  it  be \\norganized so as to encourage collaboration and  achieving higher -level goals?  \\nWhile  data  science  is  inherently  multi-disciplinary,  hence  collaborative,  in  nature,  scientists  and \\npractitioners  lack  training  in  collaboration  and  are  motivated  to  focus  on  their  objectives  and  domain. \\nWhy  would  a  bioinformaticist  (bioinformatician)  attempt  to  establish  a  data  science  method  that  goes \\nbeyond her requirements, especially as it requires an understanding of domains such as deep learning? \\nCollaboration is  also  a  significant  organizational  challenge specifically for  the  over  150 DSRIs  that  were \\nformed as a federation of organizational units each of which conduct data science activities in different \\ndomains. Like the bioinformaticist, each organization has its own objectives, budget, and investments in \\nfunding and  intellectual property. In such an environment, how does a DSRI establish strategic directions \\nand set research objectives? One proposal is through a DSRI Chief Scientific Officer (Brodie, 2018b).  \\n8 What is world -class data science research?  \\nWhile many data science groups share a passion for data science, they do not share common data \\nscience components  – principles, data, models, and methods; pipelines; data science infrastructures;  and \\ndata infrastructures. This is  understandable  given the state of data science, and the research needs of the \\nindividual groups; however, to what extent are these groups pursuing data science,  per se ? This raises our \\noriginal questions:  What is data science? and What is world-class data science research?  These questions \\nare centr al to planning and directing data science research such as in DSRIs.  \\nThere are two types of data science research, domain specific contributions and contributions to the \\ndiscipline of data science itself. Domain specific, world class data science research concerns applications \\nof  data  science  in  specific  domains  resulting  in  domain-specific  discoveries  that  are  recognized  in  its \\ndomain as being world class . There are many compelling examples, as in section 6 . To be considered data \\nscience, the research should adhere to the definition of data science, be based on some version of the \\ndata science method, use a data sci ence pipeline, and utilize the components of data science. The data \\nscience  components  or  the  data  science  method,  including  scale,  accelerating  discovering,  finding \\nsolutions  that  might  not  have  been  discovered  otherwis e,  should  be  critical  to  achieving  the  result  in \\ncomparison with other methods.  \\nEqually or even more important , world class data science research  should establish data science as a \\nscience  or  as  a  discipline  with  robust  principles,  data,  models,  and methods;  pipelines;  a  data  science \\nmethod supported by robust data science infrastructures, and data infrastructures applicable to multiple \\ndomains. Such a contribution must be proven with appropriate applications of the first type. A wonderful \\n                                                               \\n30\\n https://datascience.harvard.edu/  \\n31\\n The DSRI list that I maintain by searching the web grows continuously  - an excellent exercise for the \\nreade r. \\n32\\n https://www.bigdata.fraunhofer.de/en.html  ',\n 'example of  generalizing a domain-specific data science method is extending the network analysis method \\napplied  to  some  specific  medical  corpora  used  successfully  in  drug discovery  (Spangler et.  al.,  2014)  to \\ndomain-independent  scientific  discovery  applied  to  arbitra ry  scientific  corpora  (Nagarajan,  2015) . The \\noriginal method was implemented in three stage s, Exploration, Interpretation, and Analysis, using a tool \\ncall Knowledge Integration Toolkit ( KnIT). Exploration involved lexical analysis and text mining  of abstra cts \\nof the entire corpora up to 2003 ( 240,000)  of medical literature mentioning  kinases, a type of  protein  that \\ngoverns cell  growth,  looking for  proteins  that  govern  p53,  a  tumor  suppressor. This resulted  in  70,000 \\npapers to analyze further. Interpretation  analyzed some of the papers to produce a model of each kinase \\nand built a connected graph that represents the similarity relationship among kinases. The analysis phase \\nidentified  and  eliminated  kinases  that are  not  p53,  ultimately  resulting  in discovering  nine  kinases  with \\nthe desired properties. A retrospective search of the literature verified that seven of the nine were proven \\nempirically  to  be  tumor  suppressors  (candidates  for  cancer  drugs) in  papers  published  2003 -2013. This \\nsignificantly raised the probability that the 2 remaining kinases were as yet undiscovered candidates for \\ncancer drugs. These were world -class data science results and a magnificent example of analysis involving \\ncomplexity beyond human cognition. First and foremost, the two kinases  were accepted by the medical \\ncommunity as candidate tumor suppressors, i.e., published in medical journals. Second, the discovery was \\ndue  to  data  science  methods.  Data  science  accelerated  discovery  since  typically  one  such  cancer  drug \\ncandidate is found e very two to three years; once the KnIT model was built the candidate kinases were \\ndiscovered in approximately three months. The verification method, the retrospective analysis of cancer \\ndrug discovery 2003 -2013 was brilliant. As with most data science anal ysis, the results were probabilistic, \\ni.e., the nine candidate kinases were determined to likely candidates by the network model of the kinases, \\nhowever,  verification,  or  further  confirmation,  was  established  by  a  method  outside  data  science \\naltogether,  i.e.,  discovered  previously  published  results.  The  original  analytical  method  that  provided \\nautomated hypothesis generation (i.e., these kinases are similar) based on text mining of medical corpora \\nconcerning  proteins  was  generalized  to  automated  hypothesis  generation  based  on  text  mining  of  any \\nscientific  corpora.  While  the  first  result  was  domain-specific,  hence  an  application  of  data  science,  the \\nextension of the domain -specific method to all scientific domains was  a contribution to the science of data \\nscience. This is a higher level of world -class data science research.  \\nThe charter of every DSRI should include both domain -specific data science research and research to \\nestablish data science as a discipline. Since most DSRIs were formed from groups successf ully practicing \\ndomain-specific  data  science,  they  are  all  striving  for world  class  domain-specific  data  science.  Without \\nworld class research in data science  per se , it would be hard to argue that the DSRI contributes more than \\nthe sum of its parts. One m ight argue that lacking research into data science  per se means that the DSRI \\nhas  more  of  an  organizational or marketing  purpose  than  a  research  focus. The primary  objective  of a \\nsignificant portion of the 150 DSRIs referenced above appears to be organizat ional, e.g., to bring together \\nthe various organizations that conduct data science. In contrast, in 2012 the Irish Government established \\nInsight  Center  for  Data  Analytics  as  a  national  DSRI  to  conduct  data  science  research  and  apply  it  in \\ndomains relevant  to Ireland’s future. In doing so it set objectives much higher than bringing together data \\nscience  activities  from  its  seven  universities.  The  government  of  Ireland,  through  its  funding  agency, \\nScience Foundation Ireland (SFI), continuously eval uates Insi ght on world class data science. This includes \\nadvancing data science principles, data, models, and methods and proving their value by achieving results \\nin health and human performance, enterprises and services, smart communities and internet of things, \\nand sustainability. More challenging, however, SFI requires that Insight contributes more than the sum of \\nthe  parts,  the individual units  working  on  their  own. This contributes  to  the  science  of  data  science by \\ndeveloping principles, data models, methods, pi pelines,  and  infrastructure  that is  applicable  to multiple \\ndomains.   \\n9 Conclusions  \\nData  science  is  an  emerging  paradigm  with  the  primary  advantage  of  accelerating  discovery  of \\ncorrelations  between  variables  at  a  scale  and  speed  beyond  human  cognition  and  pre vious  discovery \\nparadigms. Data science differs paradigmatically from its predecessor scientific discovery paradigms that \\nwere  designed  to  discover causality  – Why  a  phenomenon  occurred - in  real  contexts.  Data  science  is ',\n 'designed  to  discover correlations  – What  phenomena  may  have  or  may  occur - in  data purported  to \\nrepresent  some real or imagined phenomenon . Unlike previous scientific discovery paradigms that were \\ndesigned  for  scientific  discovery  and  are  now  applied  in  many  non-scientific  domains, data  science  is \\napplicable to any domain for which adequate data is available. Hence, the potential of broad applicability \\nand  accelerating  discovery  in  any  domain  to  rapidly  reduce  the  search  space  for  solutions  holds \\nremarkable  potential  for  all  fields.  While  already  applicable  and  applied  successfully  in  many  domains, \\nthere are many challenges that must be addresses over the next decade as data science matures.  \\nMy  decade-long  experience in  data  science  suggests  that there are  no  compelling  answers  to  th e \\nquestions posed in this chapter. This is due in part to its recent emergence, it’s almost unlimited breadth \\nof applicability, and to its inherently multidisciplinary, collaborative nature.   \\nTo warrant the designation  data science , this emerging paradigm,  as a science, requires fundamental \\nprinciples and techniques applicable to all relevant domains. Since most “data science” work is domain \\nspecific,  often  model- and  method-specific,  “data  science”  does  not  yet  warrant  the  designation  of  a \\nscience. This is not a mere appeal for formalism. There are many challenges facing data science such as \\nvalidating results thereby minimizing the risks of failures. The  potential  benefits of data science, e.g., in \\naccelerating the discovery of cancer cures and solutions t o global warming, warrant establishing rigorous, \\nefficient data science principles and methods  that could change our world for the better.  \\n10 References  \\nBraschler, M., Stadelmann, T. & Stockinger, K. (Eds.) (2018). “Applied Data Science  - Lessons Learned for \\nthe Data -Driven Business”, Berlin, Heidelberg: Springer, expected 2018 \\nBrodie, M.L. (2014a) “ The First Law of Data Science: Do Umbrellas Cause Rain? ,” KDnuggets, Jun. 2014 . \\nBrodie, M.L. (2014b) “ Piketty  Revisited: Improving Economics through Data Science  – How Data Curation \\nCan Enable More Faithful Data Science (In Much Less Time) ,” KDnugge ts, Oct. 2014 . \\nBrodie, M.L. (2015a). Understanding Data Science: An Emerging Discipline for Data -Intensive Discovery, in \\nShannon Cutt (ed.), Getting Data Right: Tackling the Challenges of Big Data Volume and Variety, O’Reilly \\nMedia, Sebastopol, CA, USA, June 2015  \\nBrodie, M.L. (2015b)  Doubt and Verify:  Data Science Power Tools , KDnuggets, July 2015 . Republished on \\nODBMS.o rg. \\nBrodie, M.L. (2015c)  On Political Economy and Data Science: When A Discipline Is Not Enough , KDnuggets, \\nNovember 2015. Republished ODBMS.org November 20, 2015.  \\nBrodie, M.L. (2018a)  Why understanding truth is important in Data Science? KDnuggets, January 1, 2018. \\nRepublished Experfy.com, February 16, 2018.  \\nBrodie, M.L. (2018b). On Developing Data Science, to appear in (Braschler, et. al. 2018)  \\nCambridge  Mobile  Telematics,  (2018).  Distraction  2018:  Data  from  over  65  million  trips  shows that \\ndistracted driving is increasing, April 2, 2018.  \\nCastanedo, F. (2015). Data Preparation in the Big Data Era: Best Practices for Data Integration, O’Reilly, \\nAugust 2015.  \\nDasu, T. & Johnson, T. (2003) “Exploratory Data Mining and Cleaning,” Wiley -IEEE (2003). \\nData Science (2018), Opportunities to Transform Chemical Sciences and Engineering, A Chemical Sciences \\nRoundtable Workshop, National Academies of Science, February 27 -28, 2018  \\nDemirkan,  H.  &  Dal,  B.  (2014)  The  Data  Economy:  Why  do  so  many  analytics projects  fail?  Analytics \\nMagazine, July/August 2014.  \\nDietterich,  T.  G.  (2000,  June).  Ensemble  methods  in  machine  learning.  In  International  workshop  on \\nmultiple classifier systems (pp. 1 -15). Springer, Berlin, Heidelberg.  ',\n \"Dingus, T. A., et. al. (2016). Dri ver crash risk factors and prevalence evaluation using naturalistic driving \\ndata.  Proceedings  of  the  National  Academy  of  Sciences,  113(10),  2636–2641. \\nhttp://doi.org/10.1073/pnas.1513271113  \\nDuggan,  J.  &  Brodie,  M.  L.  (2015).  “Hephaestus:  Data  Reuse  for  Accelerating  Scientific  Discovery,”  CIDR \\n2015, Jan. 2015.  \\nEconomist (April 2017). How Germany’s Otto uses artificial intelligence,  The Economist , April 12, 2017.  \\nEconomist (May 2017). The World’s most val uable resource,  The Economist , May 4, 2017.  \\nEconomist  (January  2018)  Many  happy  returns:  new  data  reveal  long-term  investment  trends, The \\nEconomist , January 6, 2018.  \\nEconomist  (February  2018).  Economists  cannot  avoid  making  value  judgments:  Lessons  from  the \\n“repugnant” market for organs, The Economist, Feb 24, 2018.  \\nEconomist (March 2018). In algorithms we trust: How AI is spreading throughout the supply chain, The \\nEconomist, Mar 28, 2018   \\nEriksson, J., Girod, L., Hull, B., Newton, R., Madden, S., & Balakri shnan, H. (2008) The pothole patrol: using \\na mobile sensor network for road surface monitoring. In Proceedings of the 6th international conference \\non Mobile systems, applications, and services (MobiSys '08). ACM, New York, NY, USA.   \\nForrester  (2015).  Predictions  2016:  The  Path  from  Data  to  Action  for  Marketers:  How  Marketers  Will \\nElevate Systems of Insight. Forrester Research, November 9, 2015  \\nForrester  (2017).  The  Forrester  Wave:  Predictive  Analytics  and  Machine  Learning  Solutions,  Q1  2017, \\nMarch 7, 2017.  \\nGartner  G00310700  (2016)  Survey  Analysis:  Big  Data  Investments  Begin  Tapering  in  2016,  Gartner, \\nSeptember 19, 2016.  \\nGartner G00301536 (2017). 2017 Magic Quadrant for Data Science Platforms, 14 February 2017.  \\nGartner G00326671 (2017).  Critical Capabilities for Data Science Platforms, Gartner, June 7, 2017. \\nGartner G00315888 (2017) Market Guide for Data Preparation, Gartner, 14 December 2017  \\nHey,T., Tansley, S. & Tolle,K. (Eds.) (2009) The Fourth Paradigm: Data -Intensive Scientific Disc overy\\u2028Edited \\nby Microsoft Research, 2009  \\nJenkins, J. M., et. al. (2010). Caldwell, D. A., Chandrasekaran, H., Twicken, J. D., Bryson, S. T., Quintana, E. \\nV.,  et  al.  (2010).  Overview  of  the  Kepler  Science Processing  Pipeline. The  Astrophysical  Journal  Lette rs, \\n713(2), L87.  \\nLiu  J.  T.  (2012).  “Shadow  Theory,  data  model  design  for  data  integration,”  CoRR,  vol.  1209,  2012. \\narXiv:1209.2647  \\nLohr, S. (2014) For Big -Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights, New York Times, August 17, \\n2014 \\nMayo, M. (2017) Data Preparation Tips, Tricks, and Tools: An Interview with the Insiders, KDnuggets, May \\n31, 2017 \\nNagarajan, M. et al. (2015). Predicting Future Scientific Discoveries Based on a Networked Analysis of the \\nPast  Literature.  In  Proceedings  of  the  21th  ACM  SIGKDD  International  Conference  on  Knowledge \\nDiscovery and Data Mining (KDD '15). ACM, New York, NY,  USA, 2019-2028. \\nNSF  (2016).  Realizing  the Potential  of Data  Science, Final Report  from  the  National  Science  Foundation \\nComputer  and  Information  Science  and  Engineering  Advisory  Committee  Data  Science  Working  Group, \\nDecember 2016  \",\n 'Pearl,  J.  (2009a)  Causality:  Models,  Reasoning,  and  Inference,  New  York:  Cambri dge  University  Press, \\n2009. \\nPearl, J. (2009b). Epilogue: The Art and Science of Cause and Effect, In (Pearl, 2009a) pp. 401 -428 \\nPearl,  J. (2009c). \" Causal inference in statistics: An overview,\" Statistics Surveys, 3:96 --146, 2009.  \\nPiketty, T. (2014). Capit al in the 21st Century. The Belknap Press. \\nPress,  G.  (2016).  Cleaning  Big  Data:  Most  Time -Consuming,  Least  Enjoyable  Data  Science  Task,  Survey \\nSays, Forbes, May 23, 2016  \\nReimsbach -Kounatze, C. (2015), \"The Proliferation of \"Big Data\" and Implications for O fficial Statistics and \\nStatistical  Agencies:  A  Preliminary  Analysis\",  OECD  Digital  Economy  Papers,  No.  245,  OECD  Publishing, \\nParis.DOI:  http://dx.doi.org/10.1787/5js7t9wqzvg8-en  \\nSpangler,  S.  et.  al.  (2014).  Automated  hypothesis  generation  based  on  mining  scientific  literature.  In \\nProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining \\n(KDD \\'14) . ACM, New York, NY, USA, 1877 -1886. \\nSilver, D et. al. (2017). Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., et al. (2017). \\nMastering Chess and Shogi by Self -Play with a General Reinforcement Learning Algorithm. ArXiv E-Prints, \\ncs.AI. \\nSingh, G. et. al. (2007). Optimizing Workflow Data Footprint Special issue of the Scientific Programming \\nJournal dedicated to Dynamic Computational Workflows: Discovery, Optimisation and Scheduling, 2007.  \\nStoica,I, et. al. (2017). A Berkeley Vie w of Systems Challenges for AI, Technical Report No. UCB/EECS -2017-\\n159, October 16, 2017  \\nThakur, A. (2016). Approaching (Almost) Any Machine Learning Problem, The Official Blog of Kaggle.com, \\nJuly 21, 2016.  \\nVeeramachaneni,  K.  (2016)  Why  You\\'re  Not  Getting  Value  from  Your  Data  Science,  Harvard  Business \\nReview, December 7, 2016.  \\nWaller, M. A. and Fawcett, S. E. (2013), Data Science, Predictive Analytics, and Big Data: A Revolution That \\nWill Transform Supply Cha in Design and Management. J Bus Logist, 34: 77 -84. doi:10.1111/jbl.12010  \\nWinship, C., & Morgan, S. L. (1999). The Estimation of Causal Effects from Observational Data. Annu. Rev. \\nSociol., 25(1), 659 –706. http://doi.org/10.1146/annurev.soc.25.1.659   \\n View publication stats\\nView publication stats']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we starts by reading all the pages\n",
    "pdf_WiDS=[]\n",
    "pdf_onlycontent=PyPDF2.PdfFileWriter()\n",
    "pdf_onlycontent\n",
    "for p in range(PDFreader.numPages-1):\n",
    "    page=PDFreader.getPage(p+1);\n",
    "    pdf_onlycontent.addPage(page);\n",
    "    pdf_WiDS.append(page.extractText());\n",
    "#MyDSfile.close();\n",
    "pdf_output=open('WhatIsDataScienceContentOnly.pdf','wb')\n",
    "pdf_onlycontent.write(pdf_output)\n",
    "len(pdf_WiDS)\n",
    "pdf_WiDS\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "pdf_output.close()\n",
    "MyDSfile.close();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regular expressions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you are using grep in Linux, or you want to search for patterns in javascript regular expressions are always helpful.\n",
    "Regular expressions are handled using Python's built-in **re** library. See <a href=https://docs.python.org/3/library/re.html> the docs </a> for more information\n",
    "or use <a href=https://www.w3schools.com/python/python_regex.asp> regex in w3schools </a> to refresh your knowledge and provide the right code here:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the text1.txt file in a string and search for:\n",
    "    the name Eberhard\n",
    "    Donald Trump"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Searching for a substring in a file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tesla company was incorporated as Tesla Motors, Inc. on 1-7-2003, by Martin Eberhard and Marc Tarpenning.[Eberhard and Tarpenning served as CEO and CFO, respectively. Eberhard said he wanted to build \"a car manufacturer that is also a technology company\", with its core technologies as \"the battery, the computer software, and the proprietary motor\".\n",
      "\n",
      "Ian Wright was Tesla's third employee, joining a few months later.In February 2004, the company raised $7.5 million in series A funding, including $6.5 million from Elon Musk, who had received $100 million from the sale of his interest in PayPal two years earlier. Musk became the chairman of the board of directors and the largest shareholder of Tesla. J. B. Straubel joined Tesla in May 2004 as chief technical officer.\n",
      "\n",
      "A lawsuit settlement agreed to by Eberhard and Tesla in September 2009 allows all five – Eberhard, Tarpenning, Wright, Musk, and Straubel – to call themselves co-founders.\n",
      "\n",
      "On 29-06-2010, the company became a public company via an initial public offering (IPO) on NASDAQ, the first American car company to do so since the Ford Motor Company had its IPO in 1956. The company issued 13.3 million shares of common stock at a price of $17.00 per share, raising $226 million.\n"
     ]
    }
   ],
   "source": [
    "with open('text1.txt') as mystrings:\n",
    "      textstrings=mystrings.readlines();\n",
    "onestring=''\n",
    "for i in textstrings:\n",
    "    onestring=onestring+i;\n",
    "print(onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "pattern='Eberhard'\n",
    "matchE=re.search(pattern,onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(80, 88), match='Eberhard'>\n"
     ]
    }
   ],
   "source": [
    "print(matchE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 88)\n"
     ]
    }
   ],
   "source": [
    "# the Match object matchE has several attributes and methods\n",
    "print(matchE.span())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "80"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchE.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "88"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchE.end()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "pattern='Donald Trump'\n",
    "matchDT=re.search(pattern,onestring)\n",
    "#when the pattern is not found a 'None' is returned (in a notebook we get nothing if the cell is executed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But we see that the name Eberhard appears several times in the string, how can we find all the appearances?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "pattern='Eberhard'\n",
    "matchall=re.findall(pattern,onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eberhard', 'Eberhard', 'Eberhard', 'Eberhard', 'Eberhard']\n"
     ]
    }
   ],
   "source": [
    "print(matchall)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matchall) # list the total number of appearances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# how can we find the place of these appearances in the string?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 88)\n",
      "(110, 118)\n",
      "(171, 179)\n",
      "(813, 821)\n",
      "(868, 876)\n"
     ]
    }
   ],
   "source": [
    "matches=re.finditer(pattern,onestring)\n",
    "for i in matches:\n",
    "    print(i.span())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern Eberhard starts at position 80\n",
      "Pattern Eberhard starts at position 110\n",
      "Pattern Eberhard starts at position 171\n",
      "Pattern Eberhard starts at position 813\n",
      "Pattern Eberhard starts at position 868\n"
     ]
    }
   ],
   "source": [
    "# if we only are interested in the start positions:\n",
    "matches=re.finditer(pattern,onestring)\n",
    "for i in matches:\n",
    "    print(f'Pattern {i.group()} starts at position {i.start()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### More complex patterns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the text1.txt file in a string and search for:\n",
    "     a date in the following format: 15-12-1969\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "with open('text1.txt') as mystrings:\n",
    "      textstrings=mystrings.readlines();\n",
    "onestring=''\n",
    "for i in textstrings:\n",
    "    onestring=onestring+i;\n",
    "print(onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We see that the date pattern appears once in a form dd-dd-dddd where d is a digit and once as d-d-dddd\n",
    "# we unleash the power of regular expressions by using the following conventions:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Identifiers for Characters in Patterns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Example Match</th></tr>\n",
    "\n",
    "<tr ><td><span >\\d</span></td><td>A digit</td><td>file_\\d\\d</td><td>file_25</td></tr>\n",
    "\n",
    "<tr ><td><span >\\w</span></td><td>Alphanumeric</td><td>\\w-\\w\\w\\w</td><td>A-b_1</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\s</span></td><td>White space</td><td>x\\sy\\sz</td><td>x y z</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\D</span></td><td>A non digit</td><td>\\D\\D\\D</td><td>AB?</td></tr>\n",
    "\n",
    "<tr ><td><span >\\W</span></td><td>Non-alphanumeric</td><td>\\W\\W\\W\\W\\W</td><td>*-+=)</td></tr>\n",
    "\n",
    "<tr ><td><span >\\S</span></td><td>Non-whitespace</td><td>\\S\\S\\S\\S</td><td>Ext?</td></tr></table>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Quantifiers in Patterns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >+</span></td><td>Occurs one or more times</td><td>\tVersion \\w-\\w+</td><td>Version A-b1_1</td></tr>\n",
    "\n",
    "<tr ><td><span >{5}</span></td><td>Occurs exactly 5 times</td><td>\\D{5}</td><td>abcT!</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{2,6}</span></td><td>Occurs 2 to 6 times</td><td>\\d{2,6}</td><td>123</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{2,}</span></td><td>Occurs 2 or more times </td><td>\\w{2,}</td><td>anycharacters</td></tr>\n",
    "\n",
    "<tr ><td><span >*</span></td><td>Occurs zero or more times</td><td>A*B*C*</td><td>AAACC</td></tr>\n",
    "\n",
    "<tr ><td><span >?</span></td><td>Once or none</td><td>plurals?</td><td>plural</td></tr></table>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "<re.Match object; span=(60, 68), match='1-7-2003'>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#construct the date pattern and search the first date\n",
    "patterndate='\\d\\d?-\\d\\d?-\\d{4}'\n",
    "re.search(patterndate,onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "#construct the date pattern and search foe all the dates\n",
    "matches=re.finditer(patterndate,onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(60, 68), match='1-7-2003'>\n",
      "<re.Match object; span=(955, 965), match='29-06-2010'>\n"
     ]
    }
   ],
   "source": [
    "for i in matches:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose that we want to from the different dates want to store the years and the months. We can use parentheses to make groups!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "datepattern=re.compile('(\\d\\d?)-(\\d\\d?)-(\\d{4})')\n",
    "matches=re.finditer(datepattern,onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the year 2003 is important for Tesla\n",
      "the year 2010 is important for Tesla\n"
     ]
    }
   ],
   "source": [
    "# for example we print the years of the dates in the string\n",
    "for i in matches:\n",
    "    print(f'the year {i.group(3)} is important for Tesla')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the month 7 is important for Tesla\n",
      "the month 06 is important for Tesla\n"
     ]
    }
   ],
   "source": [
    "# for example we print the years of the dates in the string\n",
    "matches=re.finditer(datepattern,onestring)\n",
    "for i in matches:\n",
    "    print(f'the month {i.group(2)} is important for Tesla')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the date 1-7-2003 is important for Tesla\n",
      "the date 29-06-2010 is important for Tesla\n"
     ]
    }
   ],
   "source": [
    "# we also can print the entire match\n",
    "matches=re.finditer(datepattern,onestring)\n",
    "for i in matches:\n",
    "    print(f'the date {i.group()} is important for Tesla')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Advanced Regex Syntax\n",
    "\n",
    "##### Or operator |\n",
    "\n",
    "Use the pipe operator to have an **or** statement. For example if we want to know where in the string we come across the name of a co-founder we can use the pipe symbol:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "cofounderpattern=re.compile(\"Musk|Eberhard|Wright|Tarpenning|Straubel\")\n",
    "matches=re.finditer(cofounderpattern,onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(80, 88), match='Eberhard'>\n",
      "<re.Match object; span=(98, 108), match='Tarpenning'>\n",
      "<re.Match object; span=(110, 118), match='Eberhard'>\n",
      "<re.Match object; span=(123, 133), match='Tarpenning'>\n",
      "<re.Match object; span=(171, 179), match='Eberhard'>\n",
      "<re.Match object; span=(360, 366), match='Wright'>\n",
      "<re.Match object; span=(526, 530), match='Musk'>\n",
      "<re.Match object; span=(621, 625), match='Musk'>\n",
      "<re.Match object; span=(716, 724), match='Straubel'>\n",
      "<re.Match object; span=(813, 821), match='Eberhard'>\n",
      "<re.Match object; span=(868, 876), match='Eberhard'>\n",
      "<re.Match object; span=(878, 888), match='Tarpenning'>\n",
      "<re.Match object; span=(890, 896), match='Wright'>\n",
      "<re.Match object; span=(898, 902), match='Musk'>\n",
      "<re.Match object; span=(908, 916), match='Straubel'>\n"
     ]
    }
   ],
   "source": [
    "for match in matches:\n",
    "    print(match)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The Wildcard Character\n",
    "\n",
    "Use a \"wildcard\" as a placement that will match any character placed there. A simple period **.** will do the job. For example:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(232, 234), match='al'>\n",
      "<re.Match object; span=(572, 574), match='al'>\n",
      "<re.Match object; span=(599, 601), match='al'>\n",
      "<re.Match object; span=(766, 768), match='al'>\n",
      "<re.Match object; span=(850, 852), match='al'>\n",
      "<re.Match object; span=(857, 859), match='al'>\n",
      "<re.Match object; span=(923, 925), match='al'>\n",
      "<re.Match object; span=(1015, 1017), match='al'>\n"
     ]
    }
   ],
   "source": [
    "matches=re.finditer(r'al',onestring);\n",
    "for match in matches:\n",
    "    print(match)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that we get only three characters, to get more characters we need additional characters.E.g.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(229, 234), match='is al'>\n",
      "<re.Match object; span=(569, 574), match='e sal'>\n",
      "<re.Match object; span=(596, 601), match='ayPal'>\n",
      "<re.Match object; span=(763, 768), match='nical'>\n",
      "<re.Match object; span=(847, 852), match='09 al'>\n",
      "<re.Match object; span=(854, 859), match='ws al'>\n",
      "<re.Match object; span=(920, 925), match='o cal'>\n",
      "<re.Match object; span=(1012, 1017), match='itial'>\n"
     ]
    }
   ],
   "source": [
    "matches=re.finditer(r'...al',onestring);\n",
    "for match in matches:\n",
    "    print(match)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose we only want to find the words that ends with al"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PayPal \n",
      "technical \n",
      "initial \n"
     ]
    }
   ],
   "source": [
    "matches=re.finditer(r'\\S+al\\s|\\Sal\\.',onestring);\n",
    "for match in matches:\n",
    "    print(match.group())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Starts With and Ends With\n",
    "\n",
    "We can use the **^** to signal starts with, and the **$** to signal ends with:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ends with a number\n",
    "re.findall(r'\\d$',onestring)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    }
   ],
   "source": [
    "# Find the first character of the file if it is not a number\n",
    "match=re.findall(r'^\\D',onestring)\n",
    "print(match[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "# find the last non-punctuation character of the file\n",
    "match=re.search(r'(\\w)(\\W$)',onestring)\n",
    "print(match.group(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Exclusion\n",
    "\n",
    "To exclude characters, we can use the **^** symbol in conjunction with a set of brackets **[]**. Anything inside the brackets is excluded. For example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "phrase = \"I want 3 apples, 51 lemons and 6 oranges.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "['I',\n ' ',\n 'w',\n 'a',\n 'n',\n 't',\n ' ',\n ' ',\n 'a',\n 'p',\n 'p',\n 'l',\n 'e',\n 's',\n ',',\n ' ',\n ' ',\n 'l',\n 'e',\n 'm',\n 'o',\n 'n',\n 's',\n ' ',\n 'a',\n 'n',\n 'd',\n ' ',\n ' ',\n 'o',\n 'r',\n 'a',\n 'n',\n 'g',\n 'e',\n 's',\n '.']"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^\\d]',phrase)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "['I want ', ' apples, ', ' lemons and ', ' oranges.']"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a + sign to get the parts where each number \"breaks\" the phrase.\n",
    "\n",
    "re.findall(r'[^\\d]+',phrase)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the first things we want to do in text files is to get rid of punctuations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "test_phrase = 'This is a string! But it has punctuation. How can we remove it?'\n",
    "match=re.findall('[^!.? ]+', test_phrase)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "clean=' '.join(match)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "'This is a string But it has punctuation How can we remove it'"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Brackets for Grouping\n",
    "\n",
    "As we showed above we can use brackets to group together options, for example if we wanted to find hyphenated words:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "text = 'At the check-in there was not a merry-go-round, but my father-in-law together with my mother-in-law.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "['check-in', 'merry-go-round', 'father-in-law', 'mother-in-law']"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[\\w]+-[-[\\w]+]*',text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "##### Parentheses for Multiple Options\n",
    "\n",
    "If we have multiple options for matching some parts of the substring, we can use parentheses to list out these options. E.g.:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "# Find words that start with cat and end with one of these options: 'fish','nap', or 'claw'\n",
    "text = 'Hello, would you like some catfish?'\n",
    "text2 = \"Hello, do you like the catering?\"\n",
    "text3 = \"Hello, have you seen this caterpillar?\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "<re.Match object; span=(27, 34), match='catfish'>"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'cat(fish|ering|erpillar)',text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "<re.Match object; span=(23, 31), match='catering'>"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'cat(fish|ering|erpillar)',text2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "data": {
      "text/plain": "<re.Match object; span=(26, 37), match='caterpillar'>"
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'cat(fish|ering|erpillar)',text3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Excercise write a regular expression for discovering prices. First with dollars like $12.89 or $11. Be aware that a price at the beginning of a sentence is possible. Further we want to look for prices below $1000."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "Check out: <a href=https://docs.python.org/3/howto/regex>https://docs.python.org/3/howto/regex<a/> for more info on this topic!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}