{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_validity(word):\n",
    "    if word.is_alpha and word.text.lower() not in nlp.Defaults.stop_words:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_sample_from_file(file_name, character_count, start_position=0):\n",
    "    possible_encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    unprocessed_text = ''\n",
    "    for encoding in possible_encodings:\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=encoding) as f:\n",
    "                f.seek(start_position)\n",
    "                text = f.read(character_count)\n",
    "                unprocessed_text += text\n",
    "            doc = nlp(text)\n",
    "            # Check and remove the first token if it's not a valid word\n",
    "            if check_word_validity(doc[0]):\n",
    "                print(\"removing first token: \", doc[0])\n",
    "                doc = doc[1:]\n",
    "\n",
    "            # Check and remove the last token if it's not a valid word\n",
    "            if check_word_validity(doc[-1]):\n",
    "                print(\"removing last token: \", doc[-1])\n",
    "                doc = doc[:-1]\n",
    "            return doc, unprocessed_text\n",
    "        except UnicodeDecodeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing first token:  G\n",
      "removing first token:  utral\n",
      "removing last token:  watered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "sherlock_homes_sample, unprocessed_sherlock_homes_sample = extract_sample_from_file(os.path.join(\"data\",\"sherlock_homes.txt\"), character_count=502, start_position=10000)\n",
    "social_new_orleans_sample, unprocessed_social_new_orleans_sample = extract_sample_from_file(os.path.join(\"data\",\"social_new_orleans.txt\"), character_count=502, start_position=10000)\n",
    "the_lindsays_sample, unprocessed_the_lindsays_sample  = extract_sample_from_file(os.path.join(\"data\",\"the_lindsays.txt\"), character_count=502, start_position=10000)\n",
    "\n",
    "labeled_unprocessed_documents = {\n",
    "    'sherlock_homes_sample': unprocessed_sherlock_homes_sample,\n",
    "    'social_new_orleans_sample': unprocessed_social_new_orleans_sample,\n",
    "    'the_lindsays_sample': unprocessed_the_lindsays_sample\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with a small \"t\" woven into the texture of the paper.\n",
       "\n",
       "\"What do you make of that?\" asked Holmes.\n",
       "\n",
       "\"The name of the maker, no doubt; or his monogram, rather.\"\n",
       "\n",
       "\"Not at all. The 'G' with the small 't' stands for 'Gesellschaft,' which is the German for 'Company.' It is a customary contraction like our 'Co.' 'P,' of course, stands for 'Papier.' Now for the 'Eg.' Let us glance at our Continental Gazetteer.\" He took down a heavy brown volume from his shelves. \"Eglow, Eglonitz--here we are, Egria. It"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sherlock_homes_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ground, for we walked, and\n",
       "it was not considered far.\n",
       "\n",
       "The Farmers’ and Traders’ Bank was on Canal Street, and the family of Mr.\n",
       "Bell, the cashier, lived over the bank. There were children there and a\n",
       "governess, who went fishing with us. We rarely caught anything and had no\n",
       "use for it when we did.\n",
       "\n",
       "Sometimes I was permitted to go to market with John, way down to the\n",
       "old French Market. We had to start early, before the shops on Chartres\n",
       "Street were open, and the boys busy with scoops"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_new_orleans_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "he railway\n",
       "station, I noticed a stout dog-cart standing at the corner of a\n",
       "by-road, under a tall, straggling thorn hedge. The youth who was seated\n",
       "in it made a sign to the coachman to stop, and I was made aware that\n",
       "the dog-cart had been sent for me. I got down, and as I bade good-night\n",
       "to the cross-questioning farmer, I observed a grim smile of triumph on\n",
       "his firmly compressed lips. He evidently knew the dog-cart, and would\n",
       "now be able to trace the mysterious stranger.\n",
       "\n",
       "I and my portmanteau were "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_lindsays_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "corpus = [sherlock_homes_sample, social_new_orleans_sample, the_lindsays_sample]\n",
    "\n",
    "dictionary = corpora.Dictionary([doc.text.lower().split() for doc in corpus])\n",
    "\n",
    "corpus_bow = [dictionary.doc2bow(doc.text.lower().split()) for doc in corpus]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.604*\"the\" + 0.312*\"and\" + 0.261*\"a\" + 0.251*\"to\" + 0.222*\"i\" + 0.189*\"of\" + 0.188*\"for\" + 0.156*\"was\" + 0.147*\"we\" + 0.141*\"it\"')\n",
      "(1, '-0.319*\"and\" + -0.247*\"to\" + 0.239*\"of\" + 0.229*\"for\" + -0.192*\"i\" + 0.163*\"is\" + 0.163*\"our\" + 0.163*\"stands\" + 0.163*\"small\" + -0.159*\"was\"')\n",
      "(2, '-0.370*\"i\" + 0.289*\"we\" + -0.274*\"a\" + 0.226*\"with\" + -0.147*\"dog-cart\" + -0.147*\"made\" + 0.141*\"there\" + -0.139*\"he\" + 0.128*\"and\" + 0.097*\"for\"')\n"
     ]
    }
   ],
   "source": [
    "lsa_model = LsiModel(corpus_bow, num_topics=5, id2word=dictionary)\n",
    "\n",
    "# print all topics from the LSA model\n",
    "for topic in lsa_model.print_topics():\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.054*\"59\" + 0.037*\"68\" + 0.029*\"14\" + 0.027*\"113\" + 0.022*\"93\" + 0.019*\"63\" + 0.016*\"118\" + 0.016*\"29\" + 0.016*\"49\" + 0.016*\"65\"')\n",
      "(1, '0.017*\"59\" + 0.010*\"65\" + 0.010*\"49\" + 0.010*\"29\" + 0.009*\"14\" + 0.009*\"40\" + 0.008*\"63\" + 0.008*\"51\" + 0.008*\"36\" + 0.008*\"26\"')\n",
      "(2, '0.060*\"59\" + 0.029*\"68\" + 0.028*\"93\" + 0.027*\"14\" + 0.025*\"113\" + 0.022*\"49\" + 0.018*\"29\" + 0.017*\"118\" + 0.014*\"40\" + 0.012*\"63\"')\n",
      "(3, '0.023*\"59\" + 0.017*\"49\" + 0.015*\"29\" + 0.013*\"14\" + 0.011*\"55\" + 0.011*\"39\" + 0.011*\"18\" + 0.011*\"36\" + 0.010*\"40\" + 0.010*\"65\"')\n",
      "(4, '0.053*\"59\" + 0.024*\"29\" + 0.019*\"49\" + 0.019*\"68\" + 0.018*\"113\" + 0.017*\"14\" + 0.017*\"65\" + 0.016*\"40\" + 0.013*\"63\" + 0.012*\"118\"')\n"
     ]
    }
   ],
   "source": [
    "lda_model = LdaModel(corpus_bow, num_topics=5)\n",
    "\n",
    "# print all topics from the LDA model\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus_bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus_bow], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"straggling thorn hedge\"\n",
    "\n",
    "processed_query = nlp(query)\n",
    "query_bow = dictionary.doc2bow(processed_query.text.split())\n",
    "\n",
    "# Transform the query into the same vector space as the documents for Top2Vec\n",
    "query_vector_gensim = lsa_model[query_bow] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Document (LSA):\n",
      "Sample: sherlock_homes_sample\n",
      "Document: G\" with a small \"t\" woven into the texture of the paper.\n",
      "\n",
      "\"What do you make of that?\" asked Holmes.\n",
      "\n",
      "\"The name of the maker, no doubt; or his monogram, rather.\"\n",
      "\n",
      "\"Not at all. The 'G' with the small 't' stands for 'Gesellschaft,' which is the German for 'Company.' It is a customary contraction like our 'Co.' 'P,' of course, stands for 'Papier.' Now for the 'Eg.' Let us glance at our Continental Gazetteer.\" He took down a heavy brown volume from his shelves. \"Eglow, Eglonitz--here we are, Egria. It \n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between the query and each document for LSA\n",
    "similarity_scores_lsa = index[query_bow]\n",
    "\n",
    "most_similar_index_lsa = similarity_scores_lsa.argmax()\n",
    "\n",
    "most_similar_sample = list(labeled_unprocessed_documents.keys())[most_similar_index_lsa]\n",
    "most_similar_document_lsa = labeled_unprocessed_documents[most_similar_sample]\n",
    "\n",
    "print(\"Most Similar Document (LSA):\")\n",
    "print(f\"Sample: {most_similar_sample}\")\n",
    "print(f\"Document: {most_similar_document_lsa}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
