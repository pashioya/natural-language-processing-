{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os \n",
    "import gensim\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_validity(word):\n",
    "    if word.is_alpha and word.text.lower() not in nlp.Defaults.stop_words:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_sample_from_file(file_name, character_count, start_position=0):\n",
    "    possible_encodings = ['utf-8', 'latin-1', 'windows-1252']\n",
    "    unprocessed_text = ''\n",
    "    for encoding in possible_encodings:\n",
    "        try:\n",
    "            with open(file_name, 'r', encoding=encoding) as f:\n",
    "                f.seek(start_position)\n",
    "                text = f.read(character_count)\n",
    "                unprocessed_text += text\n",
    "            doc = nlp(text)\n",
    "            # Check and remove the first token if it's not a valid word\n",
    "            if check_word_validity(doc[0]):\n",
    "                print(\"removing first token: \", doc[0])\n",
    "                doc = doc[1:]\n",
    "\n",
    "            # Check and remove the last token if it's not a valid word\n",
    "            if check_word_validity(doc[-1]):\n",
    "                print(\"removing last token: \", doc[-1])\n",
    "                doc = doc[:-1]\n",
    "            return doc, unprocessed_text\n",
    "        except UnicodeDecodeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing first token:  G\n",
      "removing last token:  mon\n",
      "removing first token:  n\n",
      "removing last token:  uncertai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "sherlock_homes_sample, unprocessed_sherlock_homes_sample = extract_sample_from_file(os.path.join(\"data\",\"sherlock_homes.txt\"), character_count=502, start_position=10000)\n",
    "social_new_orleans_sample, unprocessed_social_new_orleans_sample = extract_sample_from_file(os.path.join(\"data\",\"social_new_orleans.txt\"), character_count=502, start_position=10000)\n",
    "the_lindsays_sample, unprocessed_the_lindsays_sample  = extract_sample_from_file(os.path.join(\"data\",\"the_lindsays.txt\"), character_count=502, start_position=10000)\n",
    "\n",
    "labeled_unprocessed_documents = {\n",
    "    'sherlock_homes_sample': unprocessed_sherlock_homes_sample,\n",
    "    'social_new_orleans_sample': unprocessed_social_new_orleans_sample,\n",
    "    'the_lindsays_sample': unprocessed_the_lindsays_sample\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" with a small \"t\" woven into the texture of the paper.\n",
       "\n",
       "\"What do you make of that?\" asked Holmes.\n",
       "\n",
       "\"The name of the maker, no doubt; or his monogram, rather.\"\n",
       "\n",
       "\"Not at all. The 'G' with the small 't' stands for 'Gesellschaft,' which is the German for 'Company.' It is a customary contraction like our 'Co.' 'P,' of course, stands for 'Papier.' Now for the 'Eg.' Let us glance at our Continental Gazetteer.\" He took down a heavy brown volume from his shelves. \"Eglow, Eglonitz--here we are, Egria. It"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sherlock_homes_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "or it when we did.\n",
       "\n",
       "Sometimes I was permitted to go to market with John, way down to the\n",
       "old French Market. We had to start early, before the shops on Chartres\n",
       "Street were open, and the boys busy with scoops watered the roadway from\n",
       "brimming gutters. John and I hurried past. Once at market we rushed from\n",
       "stall to stall, filling our basket, John forgetting nothing that had been\n",
       "ordered, and always carefully remembering one most important item, the\n",
       "saving of at least a picayune out of the market"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_new_orleans_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ", and as I bade good-night\n",
       "to the cross-questioning farmer, I observed a grim smile of triumph on\n",
       "his firmly compressed lips. He evidently knew the dog-cart, and would\n",
       "now be able to trace the mysterious stranger.\n",
       "\n",
       "I and my portmanteau were finally left on the side of the road, and\n",
       "the young man in the dog-cart civilly turned the vehicle round (with\n",
       "some difficulty on account of the narrow road), and drew up beside me,\n",
       "to save my carrying my luggage a dozen yards. At first I was a little"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_lindsays_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "corpus = [sherlock_homes_sample, social_new_orleans_sample, the_lindsays_sample]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'with', 'a', 'small', '\"', 't', '\"', 'woven', 'into', 'the', 'texture', 'of', 'the', 'paper', '.', '\\n\\n', '\"', 'What', 'do', 'you', 'make', 'of', 'that', '?', '\"', 'asked', 'Holmes', '.', '\\n\\n', '\"', 'The', 'name', 'of', 'the', 'maker', ',', 'no', 'doubt', ';', 'or', 'his', 'monogram', ',', 'rather', '.', '\"', '\\n\\n', '\"', 'Not', 'at', 'all', '.', 'The', \"'\", 'G', \"'\", 'with', 'the', 'small', \"'\", 't', \"'\", 'stands', 'for', \"'\", 'Gesellschaft', ',', \"'\", 'which', 'is', 'the', 'German', 'for', \"'\", 'Company', '.', \"'\", 'It', 'is', 'a', 'customary', 'contraction', 'like', 'our', \"'\", 'Co.', \"'\", \"'\", 'P', ',', \"'\", 'of', 'course', ',', 'stands', 'for', \"'\", 'Papier', '.', \"'\", 'Now', 'for', 'the', \"'\", 'Eg', '.', \"'\", 'Let', 'us', 'glance', 'at', 'our', 'Continental', 'Gazetteer', '.', '\"', 'He', 'took', 'down', 'a', 'heavy', 'brown', 'volume', 'from', 'his', 'shelves', '.', '\"', 'Eglow', ',', 'Eglonitz', '--', 'here', 'we', 'are', ',', 'Egria', '.', 'It']\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "data_words = [[token.text for token in doc] for doc in corpus]\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = models.phrases.Phraser(bigram)\n",
    "trigram_mod = models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "# lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.273*\"cart\" + 0.273*\"road\" + 0.273*\"dog\" + 0.150*\"now\" + 0.137*\"compress\" + 0.137*\"account\" + 0.137*\"carry\" + 0.137*\"good\" + 0.137*\"lip\" + 0.137*\"dozen\"')\n",
      "(1, '0.425*\"market\" + 0.168*\"at\" + 0.142*\"carefully\" + 0.142*\"shop\" + 0.142*\"stall\" + 0.142*\"way\" + 0.142*\"french\" + 0.142*\"item\" + 0.142*\"important\" + 0.142*\"permit\"')\n",
      "(2, '0.299*\"stand\" + 0.299*\"small\" + 0.299*\"t\" + 0.150*\"customary\" + 0.150*\"volume\" + 0.150*\"all\" + 0.150*\"let\" + 0.150*\"here\" + 0.150*\"maker\" + 0.150*\"paper\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in data_lemmatized]\n",
    "\n",
    "\n",
    "lsa_model = LsiModel(corpus_bow, num_topics=5, id2word=dictionary)\n",
    "\n",
    "# print all topics from the LSA model\n",
    "for topic in lsa_model.print_topics():\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.029*\"104\" + 0.023*\"82\" + 0.022*\"77\" + 0.017*\"21\" + 0.016*\"92\" + 0.016*\"103\" + 0.016*\"84\" + 0.016*\"90\" + 0.015*\"73\" + 0.015*\"87\"')\n",
      "(1, '0.023*\"28\" + 0.021*\"27\" + 0.018*\"53\" + 0.017*\"29\" + 0.017*\"2\" + 0.015*\"30\" + 0.014*\"0\" + 0.014*\"6\" + 0.014*\"26\" + 0.014*\"31\"')\n",
      "(2, '0.026*\"77\" + 0.025*\"82\" + 0.017*\"104\" + 0.017*\"76\" + 0.016*\"88\" + 0.015*\"111\" + 0.015*\"89\" + 0.015*\"98\" + 0.015*\"114\" + 0.015*\"93\"')\n",
      "(3, '0.011*\"29\" + 0.010*\"28\" + 0.010*\"27\" + 0.010*\"12\" + 0.010*\"6\" + 0.010*\"21\" + 0.010*\"1\" + 0.010*\"23\" + 0.010*\"32\" + 0.010*\"11\"')\n",
      "(4, '0.035*\"53\" + 0.020*\"2\" + 0.016*\"29\" + 0.014*\"55\" + 0.014*\"27\" + 0.014*\"66\" + 0.014*\"67\" + 0.014*\"47\" + 0.014*\"64\" + 0.014*\"68\"')\n"
     ]
    }
   ],
   "source": [
    "lda_model = LdaModel(corpus_bow, num_topics=5)\n",
    "\n",
    "# print all topics from the LDA model\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus_bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[corpus_bow], num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"straggling thorn hedge\"\n",
    "\n",
    "processed_query = nlp(query)\n",
    "query_bow = dictionary.doc2bow(processed_query.text.split())\n",
    "\n",
    "# Transform the query into the same vector space as the documents for Top2Vec\n",
    "query_vector_gensim = lsa_model[query_bow] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar Document (LSA):\n",
      "Sample: sherlock_homes_sample\n",
      "Document: G\" with a small \"t\" woven into the texture of the paper.\n",
      "\n",
      "\"What do you make of that?\" asked Holmes.\n",
      "\n",
      "\"The name of the maker, no doubt; or his monogram, rather.\"\n",
      "\n",
      "\"Not at all. The 'G' with the small 't' stands for 'Gesellschaft,' which is the German for 'Company.' It is a customary contraction like our 'Co.' 'P,' of course, stands for 'Papier.' Now for the 'Eg.' Let us glance at our Continental Gazetteer.\" He took down a heavy brown volume from his shelves. \"Eglow, Eglonitz--here we are, Egria. It \n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between the query and each document for LSA\n",
    "similarity_scores_lsa = index[query_bow]\n",
    "\n",
    "most_similar_index_lsa = similarity_scores_lsa.argmax()\n",
    "\n",
    "most_similar_sample = list(labeled_unprocessed_documents.keys())[most_similar_index_lsa]\n",
    "most_similar_document_lsa = labeled_unprocessed_documents[most_similar_sample]\n",
    "\n",
    "print(\"Most Similar Document (LSA):\")\n",
    "print(f\"Sample: {most_similar_sample}\")\n",
    "print(f\"Document: {most_similar_document_lsa}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
